[{"content":"","externalUrl":"https://n9o.xyz","permalink":"/users/n9o.xyz/","section":"用户列表","summary":"","title":"n9o.xyz","type":"users"},{"content":" 一、本文介绍 # 本文给大家带来的改进机制是利用我们训练好的权重文件计算FPS，同时打印每张图片所利用的平均时间，模型大小（以MB为单位），同时支持batch_size功能的选择，对于轻量化模型的读者来说，本文的内容对你一定有帮助，可以清晰帮你展示出模型速度性能的提升以及轻量化的效果（模型大小），对于以提高精度为目的的读者本文也能够帮助大家展示出现阶段的模型速度指标。所以本文的内容是十分有用的机制，对于大家发表论文来说，下面的图片为运行后的效果可以看到，该有的指标均已打印（本文内容为我独家创新，全网无第二份）。\n欢迎大家订阅我的专栏一起学习YOLO！ 二、核心代码 # 下面的为核心代码，我们将其复制粘贴到我们的ultralytics仓库的最外层目录下即可，创建一个py文件存放进去。\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import argparse import os import time import numpy as np import torch import torch.utils.data from tqdm import tqdm from ultralytics.utils.torch_utils import select_device from ultralytics.nn.tasks import attempt_load_weights import logging # 设置日志 logging.basicConfig(level=logging.INFO, format=\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) def get_weight_size(path): \u0026#34;\u0026#34;\u0026#34;获取模型权重文件的大小（以MB为单位）。\u0026#34;\u0026#34;\u0026#34; try: stats = os.stat(path) return f\u0026#39;{stats.st_size / (1024 ** 2):.1f}\u0026#39; except OSError as e: logging.error(f\u0026#34;Error getting weight size: {e}\u0026#34;) return \u0026#34;N/A\u0026#34; def warmup_model(model, device, example_inputs, iterations=200): \u0026#34;\u0026#34;\u0026#34;模型预热，准备进行高效推理。\u0026#34;\u0026#34;\u0026#34; logging.info(\u0026#34;Beginning warmup...\u0026#34;) for _ in tqdm(range(iterations), desc=\u0026#39;Warmup\u0026#39;): model(example_inputs) def test_model_latency(model, device, example_inputs, iterations=1000): \u0026#34;\u0026#34;\u0026#34;测试模型的推理延迟。\u0026#34;\u0026#34;\u0026#34; logging.info(\u0026#34;Testing latency...\u0026#34;) time_arr = [] for _ in tqdm(range(iterations), desc=\u0026#39;Latency Test\u0026#39;): if device.type == \u0026#39;cuda\u0026#39;: torch.cuda.synchronize(device) start_time = time.time() model(example_inputs) if device.type == \u0026#39;cuda\u0026#39;: torch.cuda.synchronize(device) end_time = time.time() time_arr.append(end_time - start_time) return np.mean(time_arr), np.std(time_arr) def main(opt): device = select_device(opt.device) weights = opt.weights assert weights.endswith(\u0026#39;.pt\u0026#39;), \u0026#34;Model weights must be a .pt file.\u0026#34; model = attempt_load_weights(weights, device=device, fuse=True) model = model.to(device).fuse() example_inputs = torch.randn((opt.batch, 3, *opt.imgs)).to(device) if opt.half: model = model.half() example_inputs = example_inputs.half() warmup_model(model, device, example_inputs, opt.warmup) mean_latency, std_latency = test_model_latency(model, device, example_inputs, opt.testtime) logging.info(f\u0026#34;Model weights: {opt.weights} Size: {get_weight_size(opt.weights)}M \u0026#34; f\u0026#34;(Batch size: {opt.batch}) Latency: {mean_latency:.5f}s ± {std_latency:.5f}s \u0026#34; f\u0026#34;FPS: {1 / mean_latency:.1f}\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: parser = argparse.ArgumentParser(description=\u0026#34;Test YOLOv8 model performance.\u0026#34;) parser.add_argument(\u0026#39;--weights\u0026#39;, type=str, default=\u0026#39;yolov8n.pt\u0026#39;, help=\u0026#39;trained weights path\u0026#39;) parser.add_argument(\u0026#39;--batch\u0026#39;, type=int, default=1, help=\u0026#39;total batch size for all GPUs\u0026#39;) parser.add_argument(\u0026#39;--imgs\u0026#39;, nargs=\u0026#39;+\u0026#39;, type=int, default=[640, 640], help=\u0026#39;image sizes [height, width]\u0026#39;) parser.add_argument(\u0026#39;--device\u0026#39;, default=\u0026#39;0\u0026#39;, help=\u0026#39;cuda device, i.e. 0 or 0,1,2,3 or cpu\u0026#39;) parser.add_argument(\u0026#39;--warmup\u0026#39;, default=200, type=int, help=\u0026#39;warmup iterations\u0026#39;) parser.add_argument(\u0026#39;--testtime\u0026#39;, default=1000, type=int, help=\u0026#39;test iterations for latency\u0026#39;) parser.add_argument(\u0026#39;--half\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;use FP16 mode for inference\u0026#39;) opt = parser.parse_args() main(opt) 三、参数讲解 # 本文涉及到的参数共有如下。\n参数讲解如下 1weights对应的权重文件地址2batch计算多少个图片的FPS3imgs图片的大小默认为6404device推理的设备选择，默认为GPU：05warmup预测轮次，预热阶段目的是让模型运行在一个更稳定的状态，提高模型推理性能6testtime参数定义了进行性能评估时执行的推理轮次数，这个参数使得性能测试更加准确和可靠。通过在足够多的轮次上测量推理时间，可以减少偶然因素的影响，并获得一个更加稳定和准确的性能指标7half推理精度这个不多介绍了大家都懂，FPS16更快 四、运行文件和使用方法 # 此文件的使用方式比较特殊，我们首先需要配置好其中的参数（第一次适用建议大家用官方的权重文件），配置好之后我们需要通过命令行（终端）通过python 你给文件起的名字.py 的方式运行！！！注意不通过这种方式会报错！！\npython 你给文件起的名字.py 看下图大家就知道如何使用了。 我的默认参数都在代码中，大家可以看到我用的都是官方默认的，在3070上FPS可以达到170+（具体高低和你的设备有关，大家发论文的适合也要标明自己的实验设备，然后在相同的实验设备上改进的模型和基础模型的FPS提供了多少。）\n五、效果展示 # 下面是效果展示，可以看到所有的参数均已打印。\n","date":"2024-10-03","externalUrl":null,"permalink":"/docs/yolov9/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e一、本文介绍 \n    \u003cdiv id=\"%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e本文给大家带来的改进机制是\u003cstrong\u003e利用我们训练好的权重文件计算FPS，\u003cstrong\u003e同时打印每张图片所利用的平均时间，模型大小（以MB为单位），同时支持batch_size功能的选择，对于轻量化模型的读者来说，本文的内容对你一定有帮助，可以清晰帮你展示出模型速度性能的提升以及轻量化的效果（模型大小），对于以提高精度为目的的读者本文也能够帮助大家展示出现阶段的模型速度指标。所以本文的内容是十分有用的机制，对于大家发表论文来说，下面的图片为运行后的效果可以看到，该有的指标均已打印\u003c/strong\u003e（本文内容为我独家创新，全网无第二份）。\u003c/strong\u003e\u003c/p\u003e","title":"FPS的计算","type":"docs"},{"content":" 一、YOLOv8的简介 # YOLO（You Only Look Once）系列算法因其高效、准确等特点而备受瞩目。由2023年Ultralytics公司发布了YOLO的最新版本YOLOv8是结合前几代YOLO的基础上的一个融合改进版。\n本文YOLOv8网络结构/环境搭建/数据集获取/训练/推理/验证/导出/部署，从网络结构的讲解从模型的网络结构讲解到模型的部署都有详细介绍，同时在本专栏中还包括YOLOv8模型系列的改进包括个人提出的创新点，传统卷积、注意力机制、损失函数的修改教程，能够帮助你的论文获得创新点。\n二、YOLOv8相对于Yolov5的核心改动 # 从YOLOv8的网络结构可以看出,其延用了YOLOv5的网络结构思想，包括基于CSP（紧凑和分离）的骨干网络(backbone)和Neck部分的设计，以及对于不同尺度模型的考虑。\n改进总结：\nBackbone的改进：使用C2f模块代替C3模块，进一步轻量化，同时保持了CSP的思想，同时采用了SPPF模块。\nPAN-FPN的改进：保留了PAN的思想，但删除了上采样阶段中的卷积结构，同时将C3模块替换为C2f模块。\nDecoupled-Head的引入：采用了Decoupled-Head的思想，使得网络的训练和推理更加高效。\nAnchor-Free的思想：抛弃了Anchor-Base，采用了Anchor-Free的思想。\n损失函数的改进：采用VFL Loss作为分类损失，同时使用DFL Loss和CIOU Loss作为回归损失。\n样本匹配方式的改进：采用了Task-Aligned Assigner匹配方式。\n这些改进使得YOLOv8在目标检测方面具有更高的精度和更快的速度，同时保持了轻量化的特点。\n具体来说，YOLOv8的Backbone部分使用了C2f模块来替代了YOLOv5中的C3模块，实现了进一步的轻量化。同时，它也保留了YOLOv5等架构中使用的SPPF（空间金字塔池化）模块。\n在PAN-FPN（路径聚合网络-特征金字塔网络）部分，虽然YOLOv8依旧采用了PAN的思想，但是在结构上，它删除了YOLOv5中PAN-FPN上采样阶段中的卷积结构，并将C3模块替换为了C2f模块。\n这些改进使得YOLOv8在保持了YOLOv5网络结构的优点的同时，进行了更加精细的调整和优化，提高了模型在不同场景下的性能。\n三、YOLOv8的网络结构 # YOLOv8的网络结构主要由以下三个大部分组成：\nBackbone：它采用了一系列卷积和反卷积层来提取特征，同时也使用了残差连接和瓶颈结构来减小网络的大小和提高性能。该部分采用了C2f模块作为基本构成单元，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。\nNeck：它采用了多尺度特征融合技术，将来自Backbone的不同阶段的特征图进行融合，以增强特征表示能力。具体来说，YOLOv8的Neck部分包括一个SPPF模块、一个PAA模块和两个PAN模块。\nHead：它负责最终的目标检测和分类任务，包括一个检测头和一个分类头。检测头包含一系列卷积层和反卷积层，用于生成检测结果；分类头则采用全局平均池化来对每个特征图进行分类。\n下面我们来针对于YOLOv8的三个组成部分进行详细讲解。\n3.1 Backbone # 由最上面的YOLOv8网络结构图我们可以看出在其中的Backbone部分，由5个卷积模块和4个C2f模块和一个SPPF模块组成，\n(其中浅蓝色为卷积模块,黄色为C2f模块深蓝色为SPPF模块 )\n如果上图看的不够直观,我们来看一下YOLOv8的文件中的yaml文件,看一下它backbone部分的结构组成部分，会更加直观。 backbone: # [from, repeats, module, args] - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4 - [-1, 3, C2f, [128, True]] - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 - [-1, 6, C2f, [256, True]] - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 - [-1, 6, C2f, [512, True]] - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 - [-1, 3, C2f, [1024, True]] - [-1, 1, SPPF, [1024, 5]] # 9 上面的部分就是YOLOv8的yaml文件的Backbone部分，可以看到其由5个Conv模块，四个C2f模块以及一个SPPF模块组成，下面我们来根据每个模块的组成来进行讲解。\n3.1.1 卷积模块(Conv) # 在其中卷积模块的结构主要为下图\n在其中主要结构为一个2D的卷积一个BatchNorm2d和一个SiLU激活函数，整个卷积模块的作用为：\n降采样：每个卷积模块中的卷积层都采用步长为2的卷积核进行降采样操作，以减小特征图的尺寸并增加通道数。 非线性表示：每个卷积层之后都添加了Batch Normalization（批标准化）层和ReLU激活函数，以增强模型的非线性表示能力。 在其中Batch Normalization（批标准化）是深度学习中常用的一种技术，用于加速神经网络的训练。Batch Normalization通过对每个小批量数据进行标准化，使得神经网络在训练过程中更加稳定，可以使用更高的学习率，并且减少了对初始化权重的依赖。Batch Normalization的基本思想是：对每个小批量数据进行标准化，使得每个特征的均值为0，方差为1，然后再通过一个可学习的缩放因子和平移因子来调整数据的分布，从而使得神经网络更容易训练。\n3.1.2 C2f模块 # 在YOLOv8的网络结构中C2f模块算是YOLOv8的一个较大的改变，与YOLOv5的C3模块相比，C2f模块具有更少的参数量和更优秀的特征提取能力。下图为C2f的内部网络结构图。\n在C2f模块中我们可以看到输入首先经过一个k=1，s=1，p=0，c=c_out的卷积模块进行了处理，然后经过一个split处理**(在这里split和后面的concat的组成其实就是所谓的残差模块处理)**经过数量为n的DarknetBottleneck模块处理以后将残差模块和主干模块的结果进行Concat拼接在经过一个卷积模块处理进行输出。 在其中提到的残差连接（residual connections）是一种用于构建深层神经网络的技术。它的核心思想是通过跳过层级连接来传递残差或误差。\n在传统的神经网络中，信息流通过一层层的网络层，每一层都通过非线性激活函数进行转换和提取特征。然而，随着神经网络的加深，可能会出现\u0026quot;梯度消失\u0026quot;或\u0026quot;梯度爆炸\u0026quot;的问题，导致网络收敛困难或性能下降。\n残差连接通过引入跨层级的连接，将输入的原始信息直接传递到后续层级，以解决梯度消失和爆炸问题。具体而言，它将网络的输入与中间层的输出相加，形成了一个\u0026quot;捷径\u0026quot;或\u0026quot;跳跃连接\u0026quot;，从而允许梯度更容易地传播。\n数学上，假设我们有一个输入x，通过多个网络层进行处理后得到预测值H(x)。那么残差连接的表达式为：\nF(x) = H(x) + x\n其中，F(x)为残差块的输出，H(x)为经过一系列网络层处理后得到的特征表示，x为输入直接连接到残差块中的跳跃连接。\n通过残差连接，网络可以更容易地学习残差或误差，从而使网络更深层次的特征表达更准确。这对于训练深层神经网络非常有用，可以提高网络的性能和收敛速度。\n在C2f模块中用到的DarknetBottleneck模块其中使用多个3x3卷积核进行卷积操作，提取特征信息。同时其具有add是否进行残差链接的选项。\n其实整个C2f模块就是一个改良版本的Darknet\n首先，使用1x1卷积核将输入通道数减少到原来的1/2，以减少计算量和内存消耗。\n然后，使用多个3x3卷积核进行卷积操作，提取特征信息。\n接着，使用残差链接，将输入直接加到输出中，从而形成了一条跨层连接。\n接着，再次使用1x1卷积核恢复特征图的通道数。\nSPPF模块 YOLOv8的SPPF模块相对于YOLOv5的SPPF模块并没有任何的改变。\n3.2 Neck # YOLOv8的Neck部分是该模型中的一个关键组件，它在特征提取和融合方面起着重要作用。Neck的详细描述如下：\nNeck部分主要起到一个特征融合的操作, YOLOv8的Neck部分依然采用PAN-FPN的思想，下图的a，b，c为一个Neck部分的流程示意图。\n整个Neck部分的步骤如下：：将特征提取网络(Backbone)的输出P3，P4，P5输入进PAN-FPN网络结构，使得多个尺度的特征图进行融合；将P5经过上采样与P4进行融合得到F1，将F1经过C2f层和一次上采样与P3进行融合得到T1，将T1经过一次卷积层与F1经过融合得到F2，将F2经过一次C2f层得到T2，将T2经过一次卷积层与P5融合得到F3，将F3经过一次C2f层得到T3，最终得到T1、T2、T3就是整个Neck的产物； 上述过程可以描述为下图，我在图片上做了一些标准方便理解。\n上述的过程可以在代码部分看到,我们同样看YOLOv8的yaml文件，能够更直观的看到这个步骤,大家可以看代码同时对应图片来进行分析:\nhead: - [-1, 1, nn.Upsample, [None, 2, \u0026#39;nearest\u0026#39;]] - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 3, C2f, [512]] # 12 - [-1, 1, nn.Upsample, [None, 2, \u0026#39;nearest\u0026#39;]] - [[-1, 4], 1, Concat, [1]] # cat backbone P3 - [-1, 3, C2f, [256]] # 15 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 12], 1, Concat, [1]] # cat head P4 - [-1, 3, C2f, [512]] # 18 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 9], 1, Concat, [1]] # cat head P5 - [-1, 3, C2f, [1024]] # 21 (P5/32-large) Neck部分的整体功能的详细分析如下:\n1. Neck的作用：\nNeck部分在YOLOv8模型中负责对来自Backbone的特征进行进一步处理和融合，以提高目标检测的准确性和鲁棒性。它通过引入不同的结构和技术，将多尺度的特征图进行融合，以便更好地捕捉不同尺度目标的信息。\n2. 特征金字塔网络（Feature Pyramid Network, FPN）：\nYOLOv8的Neck部分通常采用特征金字塔网络结构，用于处理来自Backbone的多尺度特征图。FPN通过在不同层级上建立特征金字塔，使得模型能够在不同尺度上进行目标检测。它通过上采样和下采样操作，将低层级的细节特征与高层级的语义特征进行融合，以获取更全面和丰富的特征表示。\n3. 特征融合（Feature Fusion）：\nNeck部分还包括特征融合的操作，用于将来自不同层级的特征进行融合。这种特征融合有助于提高模型对目标的检测准确性，尤其是对于不同尺度的目标。\n4. 上采样和下采样：\nNeck部分通常会使用上采样和下采样操作，以调整特征图的尺度和分辨率。上采样操作可以将低分辨率的特征图放大到与高分辨率特征图相同的尺寸，以保留更多的细节信息。而下采样操作则可以将高分辨率的特征图降低尺寸，以减少计算量和内存消耗。\nYOLOv8的Neck部分通过特征金字塔网络和特征融合等操作，有效地提取和融合多尺度的特征，从而提高了目标检测的性能和鲁棒性。这使得模型能够更好地适应不同尺度和大小的目标，并在复杂场景下取得更准确的检测结果。\nPAN-FPN（具有特征金字塔网络的路径聚合网络）是一种用于计算机视觉中对象检测的神经网络架构。它将特征金字塔网络（FPN）与路径聚合网络（PAN）相结合，以提高目标检测的准确性和效率。\nFPN 用于从不同比例的图像中提取特征，而 PAN 用于跨网络的不同层聚合这些特征。这允许网络检测不同大小和分辨率的对象，并处理具有多个对象的复杂场景。\n3.3 Head # 如果Backbone和Neck部分可以理解为准备工作，那么Head部分就是收获的部分，经过前面的准备工作我们得到了Neck部分的输出T1、T2、T3分别代表不同层级的特征图，Head部分就是对这三个特征图进行处理以产生模型的的输出结果的一个过程。\nYOLOv8的Head部分我们先来看一下它的网络结构。\n可以看到在YOLOv8的Head部分，体现了最核心的改动——\u0026gt;解耦头(Decoupled-Head)，顾名思义就是将原先的一个检测头分解成两个部分。\n在Head部分的三个解耦头分别对应着Neck部分的特征图输出T1、T2、T3。、\n解耦头的工作流程是：\n将网络得到的特征图T1，T2，T3分别输入解耦头头进行预测，检测头的结构如下图所示其中包含4个3×3卷积与2个1×1卷积，同时在检测头的回归分支中添加WIOU损失函数如图4所示，回归头部需要计算预测框与真实框之间的位置偏移量，然后将偏移量送入回归头部进行损失计算，然后输出一个四维向量，分别表示目标框的左上角坐标x、y和右下角坐标x、y。分类头部针对于每个Anchor Free提取的候选框对其进行RoI Pooling和卷积操作得到一个分类器输出张量每个位置上的值表示该候选框属于每个类别的概率，在最后通过极大值抑制方式筛选出最终的检测结果 我们再从YOLOv8的yaml文件来看Head部分的作用\n可以看到检测头部分的输出为15,18，21分别对应着Neck部分的三个输出特征图。 到此YOLOv8的网络结构部分讲解就已经完成，下面我们来看如何利用YOLOv8进行训练操作。 四、环境搭建 # 在我们配置好环境之后，在之后模型获取完成之后，我们可以进行配置的安装我们可以在命令行下输入如下命令进行环境的配置。\npip install -r requirements.txt 输入如上命令之后我们就可以看到命令行在安装模型所需的库了。 五、数据集获取 # 我在上面随便下载了一个 数据集用它导出yolov8的数据集，以及自动给转换成txt的格式yaml文件也已经配置好了，我们直接用就可以。 六、模型获取 # 到这里假设你已经搭建好了环境和有了数据集，那么我们就可以进行模型的下载，因为yolov8目前还存在BUG并不稳定随时都有可能进行更新，所以不推荐大家通过其它的途径下载，最好通过下面的方式进行下载。\n我们可以直接在终端命令下\n(PS：这里需要注意的是我们需要在你总项目文件目录下输入这个命令，因为他会下载到当前目录下)\npip install ultralytics 如果大家去github上直接下载zip文件到本地可能会遇到报错如下，识别不了yolo命令，所以推荐大家用这种方式下载，\n七、模型训练 # 我们来看一下主要的ultralytics目录结构，\n我门打开cfg目录下的default.yaml文件可以配置模型的参数，\n在其中和模型训练有关的参数及其解释如下:\n参数名 输入类型 参数解释 0 task str YOLO模型的任务选择，选择你是要进行检测、分类等操作 1 mode str YOLO模式的选择，选择要进行训练、推理、输出、验证等操作 2 model str/optional 模型的文件，可以是官方的预训练模型，也可以是训练自己模型的yaml文件 3 data str/optional 模型的地址，可以是文件的地址，也可以是配置好地址的yaml文件 4 epochs int 训练的轮次，将你的数据输入到模型里进行训练的次数 5 patience int 早停机制，当你的模型精度没有改进了就提前停止训练 6 batch int 我们输入的数据集会分解为多个子集，一次向模型里输入多少个子集 7 imgsz int/list 输入的图片的大小，可以是整数就代表图片尺寸为int*int，或者list分别代表宽和高[w，h] 8 save bool 是否保存模型以及预测结果 9 save_period int 在训练过程中多少次保存一次模型文件,就是生成的pt文件 10 cache bool 参数cache用于控制是否启用缓存机制。 11 device int/str/list/optional GPU设备的选择：cuda device=0 or device=0,1,2,3 or device=cpu 12 workers int 工作的线程，Windows系统一定要设置为0否则很可能会引起线程报错 13 name str/optional 模型保存的名字，结果会保存到\u0026rsquo;project/name\u0026rsquo; 目录下 14 exist_ok bool 如果模型存在的时候是否进行覆盖操作 15 prepetrained 7.1 训练的三种方式 # 7.1.1 方式一 # 我们可以通过命令直接进行训练在其中指定参数，但是这样的方式，我们每个参数都要在其中打出来。命令如下:\nyolo task=detect mode=train model=yolov8n.pt data=data.yaml batch=16 epochs=100 imgsz=640 workers=0 device=0 需要注意的是如果你是Windows系统的电脑其中的Workers最好设置成0否则容易报线程的错误。\n7.1.2 方式二（推荐） # 通过指定cfg直接进行训练，我们配置好ultralytics/cfg/default.yaml这个文件之后，可以直接执行这个文件进行训练，这样就不用在命令行输入其它的参数了。\nyolo cfg=ultralytics/cfg/default.yaml 7.1.3 方式三 # 我们可以通过创建py文件来进行训练，这样的好处就是不用在终端上打命令，这也能省去一些工作量，我们在根目录下创建一个名字为run.py的文件，在其中输入代码\nfrom ultralytics import YOLO model = YOLO(\u0026#34;权重的地址\u0026#34;) data = \u0026#34;文件的地址\u0026#34; model.train(data=data, epochs=100, batch=16) 无论通过上述的哪一种方式在控制台输出如下图片的内容就代表着开始训练成功了！\n八、模型验证/测试 # 参数名 类型 参数讲解 1 val bool 用于控制是否在训练过程中进行验证/测试。 2 split str 用于指定用于验证/测试的数据集划分。可以选择 \u0026lsquo;val\u0026rsquo;、\u0026rsquo;test\u0026rsquo; 或 \u0026rsquo;train\u0026rsquo; 中的一个作为验证/测试数据集 3 save_json bool 用于控制是否将结果保存为 JSON 文件 4 save_hybird bool 用于控制是否保存标签和附加预测结果的混合版本 5 conf float/optional 用于设置检测时的目标置信度阈值 6 iou float 用于设置非极大值抑制（NMS）的交并比（IoU）阈值。 7 max_det int 用于设置每张图像的最大检测数。 8 half bool 用于控制是否使用半精度（FP16）进行推断。 9 dnn bool ，用于控制是否使用 OpenCV DNN 进行 ONNX 推断。 10 plots bool 用于控制在训练/验证过程中是否保存绘图结果。 验证我们划分的验证集/测试集的情况，也就是评估我们训练出来的best.pt模型好与坏\nyolo task=detect mode=val model=best.pt data=data.yaml device=0 九、模型推理 # 我们训练好自己的模型之后，都会生成一个模型文件,保存在你设置的目录下,当我们再次想要实验该模型的效果之后就可以调用该模型进行推理了，我们也可以用官方的预训练权重来进行推理。\n推理的方式和训练一样我们这里就选一种来进行举例其它的两种方式都是一样的操作只是需要改一下其中的一些参数即可:\n参数讲解\n参数名 类型 参数讲解 0 source str/optinal 用于指定图像或视频的目录 1 show bool 用于控制是否在可能的情况下显示结果 2 save_txt bool 用于控制是否将结果保存为 .txt 文件 3 save_conf bool 用于控制是否在保存结果时包含置信度分数 4 save_crop bool 用于控制是否将带有结果的裁剪图像保存下来 5 show_labels bool 用于控制在绘图结果中是否显示目标标签 6 show_conf bool 用于控制在绘图结果中是否显示目标置信度分数 7 vid_stride int/optional 用于设置视频的帧率步长 8 stream_buffer bool 用于控制是否缓冲所有流式帧（True）或返回最新的帧（False） 9 line_width int/list[int]/optional 用于设置边界框的线宽度，如果缺失则自动设置 10 visualize bool 用于控制是否可视化模型的特征 11 augment bool 用于控制是否对预测源应用图像增强 12 agnostic_nms bool 用于控制是否使用无关类别的非极大值抑制（NMS） 13 classes int/list[int]/optional 用于按类别筛选结果 14 retina_masks bool 用于控制是否使用高分辨率分割掩码 15 boxes bool 用于控制是否在分割预测中显示边界框。 yolo task=detect mode=predict model=best.pt source=images device=0 这里需要需要注意的是我们用模型进行推理的时候可以选择照片也可以选择一个视频的格式都可以。支持的视频格式有 MP4（.mp4）：这是一种常见的视频文件格式，通常具有较高的压缩率和良好的视频质量\nAVI（.avi）：这是一种较旧但仍广泛使用的视频文件格式。它通常具有较大的文件大小\nMOV（.mov）：这是一种常见的视频文件格式，通常与苹果设备和QuickTime播放器相关\nMKV（.mkv）：这是一种开放的多媒体容器格式，可以容纳多个视频、音频和字幕轨道\nFLV（.flv）：这是一种用于在线视频传输的流式视频文件格式\n十、模型输出 # 当我们进行部署的时候可以进行文件导出，然后在进行部署。\nYOLOv8支持的输出格式有如下\n1. ONNX（Open Neural Network Exchange）：ONNX 是一个开放的深度学习模型表示和转换的标准。它允许在不同的深度学习框架之间共享模型，并支持跨平台部署。导出为 ONNX 格式的模型可以在支持 ONNX 的推理引擎中进行部署和推理。\n2. TensorFlow SavedModel：TensorFlow SavedModel 是 TensorFlow 框架的标准模型保存格式。它包含了模型的网络结构和参数，可以方便地在 TensorFlow 的推理环境中加载和使用。\n3. PyTorch JIT（Just-In-Time）：PyTorch JIT 是 PyTorch 的即时编译器，可以将 PyTorch 模型导出为优化的 Torch 脚本或 Torch 脚本模型。这种格式可以在没有 PyTorch 环境的情况下进行推理，并且具有更高的性能。\n4. Caffe Model：Caffe 是一个流行的深度学习框架，它使用自己的模型表示格式。导出为 Caffe 模型的文件可以在 Caffe 框架中进行部署和推理。\n5. TFLite（TensorFlow Lite）：TFLite 是 TensorFlow 的移动和嵌入式设备推理框架，支持在资源受限的设备上进行高效推理。模型可以导出为 TFLite 格式，以便在移动设备或嵌入式系统中进行部署。\n6. Core ML（Core Machine Learning）：Core ML 是苹果的机器学习框架，用于在 iOS 和 macOS 上进行推理。模型可以导出为 Core ML 格式，以便在苹果设备上进行部署。\n这些格式都提供了不同的优势和适用场景。选择合适的导出格式应该考虑到目标平台和部署环境的要求，以及所使用的深度学习框架的支持情况。\n模型输出的参数有如下\n参数名 类型 参数解释 0 format str 导出模型的格式 1 keras bool 表示是否使用Keras 2 optimize bool 用于在导出TorchScript模型时进行优化，以便在移动设备上获得更好的性能 3 int8 bool 用于在导出CoreML或TensorFlow模型时进行INT8量化 4 dynamic bool 用于在导出CoreML或TensorFlow模型时进行INT8量化 5 simplify bool 用于在导出ONNX模型时进行模型简化 6 opset int/optional 用于指定导出ONNX模型时的opset版本 7 workspace int 用于指定TensorRT模型的工作空间大小，以GB为单位 8 nms bool 用于在导出CoreML模型时添加非极大值抑制（NMS） 命令行命令如下: yolo task=detect mode=export model=best.pt format=onnx ","date":"2024-10-03","externalUrl":null,"permalink":"/docs/alexnet/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e一、YOLOv8的简介 \n    \u003cdiv id=\"%E4%B8%80yolov8%E7%9A%84%E7%AE%80%E4%BB%8B\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%80yolov8%E7%9A%84%E7%AE%80%E4%BB%8B\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eYOLO（You Only Look Once）系列算法因其高效、准确等特点而备受瞩目。由2023年Ultralytics公司发布了YOLO的\u003cstrong\u003e最新版本YOLOv8是结合前几代YOLO的基础上的一个融合改进版\u003c/strong\u003e。\u003c/p\u003e","title":"YOLOv8简介","type":"docs"},{"content":" 一、本文介绍 # Hello，大家好这次给大家带来的不是改进，是整个YOLOv8项目的分析，整个系列大概会更新7-10篇左右的文章，从项目的目录到每一个功能代码的都会进行详细的讲解，同时YOLOv8改进系列也突破了三十篇文章，最后预计本专栏持续更新会在年底更新上百篇的改进教程， 所以大家如果没有订阅专栏可以提前订阅以下。下面开始进行YOLOv8逐行解析的第一篇——项目目录构造分析\n二、项目目录构造分析 # 开始之前先把源代码的地址分析给大家-\u0026gt;\n官方代码地址：YOLO仓库下载地址\n下面的图片是我们从仓库上下载整个打开之后的图片，左边的部分是文件，右面呢就是展示窗口。 下面的是文件部分的清晰截图-\u0026gt;\n下面我们来逐个分析左边的文件各个都是什么作用-\u0026gt;\n2.1 .github # 该目录包含以下内容：\nISSUE_TEMPLATE：提供不同类型的问题报告模板，包括 bug-report.yml、config.yml、feature-request.yml和 question.yml。这些模板帮助用户以结构化的方式报告错误、提出功能请求或提问。\nworkflows：包含多个工作流文件，如 ci.yml（持续集成）、cla.yml（贡献者许可协议）、codeql.yml（代码质量检查）、docker.yml（Docker配置）、greetings.yml（自动问候新贡献者）、links.yml、publish.yml（自动发布）、stale.yml（处理陈旧问题）。\ndependabot.yml（自动依赖更新）\n这些文件共同支持项目的自动化管理，包括代码质量保证、持续集成和部署、社区互动和依赖项维护。\n2.2 docker # 该目录包含以下内容：\ndocker 目录包含多个 Dockerfile，每个文件都是为不同环境或平台配置的，例如：\nDockerfile: 主要的Docker配置文件，用于构建项目的默认Docker镜像。 Dockerfile-arm64: 针对ARM64架构的设备（如某些类型的服务器或高级嵌入式设备）定制的Docker配置。 Dockerfile-conda: 使用Conda包管理器配置环境的Docker配置文件。 Dockerfile-cpu: 为不支持GPU加速的环境配置的Docker配置文件。 Dockerfile-jetson: 专为NVIDIA Jetson平台定制的Docker配置。 Dockerfile-python: 可能是针对纯Python环境的简化Docker配置。 Dockerfile-runner: 可能用于配置持续集成/持续部署（CI/CD）运行环境的Docker配置。 这些配置文件是用来部署用的，用户可以根据自己的需要选择合适的环境来部署和运行项目。\n2.3 docs # docs目录通常用于存放文档资料，包括多种语言的翻译。例如，此目录下有多个文件夹，每个文件夹代表一种语言（如en代表英语文档）。除此之外，还有几个重要的Python脚本和配置文件给大家说一下：\nbuild_docs.py：一个Python脚本，用于自动化构建和编译文档的过程。\nmkdocs.yml：MkDocs配置文件，用于指定文档网站的结构和设置。\n以mkdocs_es.yml为例，这是用于构建西班牙语文档的MkDocs配置文件。类似的，mkdocs_zh.yml用于构建中文文档。所以这些文档其实和我们学习YOLOv8没啥太大的关系，大家了解以下就可以了。\n2.4 examples # 在examples文件夹中，大家可以找到不同编程语言和平台的YOLOv8实现示例：\nYOLOv8-CPP-Inference：包含C++语言实现的YOLOv8推理示例，内有CMakeLists.txt（用于项目构建的CMake配置文件），inference.cpp和inference.h（推理相关的源代码和头文件），main.cpp（主程序入口）以及README.md（使用说明）。\nYOLOv8-ONNXRuntime：提供Python语言与ONNX Runtime结合使用的YOLOv8推理示例，其中main.py是主要的脚本文件，README.md提供了如何使用该示例的指南。\nYOLOv8-ONNXRuntime-CPP：与上述ONNX Runtime类似，但是是用C++编写的，包含了相应的CMakeLists.txt，inference.cpp，inference.h和main.cpp文件，以及用于解释如何运行示例的README.md。\n每个示例都配有相应的文档，是当我们进行模型部署的时候在不同环境中部署和使用YOLOv8的示例。\n2.5 tests # tests目录包含了项目的自动化测试脚本，每个脚本针对项目的不同部分进行测试：\nconftest.py：包含测试配置选项或共享的测试助手函数。\ntest_cli.py：用于测试命令行界面（CLI）的功能和行为。\ntest_cuda.py：专门测试项目是否能正确使用NVIDIA的CUDA技术，确保GPU加速功能正常。\ntest_engine.py：测试底层推理引擎，如模型加载和数据处理等。\ntest_integrations.py：测试项目与其他服务或库的集成是否正常工作。\ntest_python.py：用于测试项目的Python API接口是否按预期工作。\n这些测试脚本确保大家在改进了文件之后更新或添加的新功能后仍能运行的文件。\n2. 6 runs # 这个文件我们在上面目录构造没有看到是因为，这是我们成功训练了一次模型之后生成的文件，里面保存我们每一次训练之后的各种信息。\n下面的是训练成功之后的一个完整保存文件:\n2.6 utlralytics(重点) # 上面讲的大部分文件其实对于大部分读者都用不上，这里的utralytics文件才是重点，包含了YOLOv8的所有功能都集成在这个文件目录下面，这里我只介绍每一个目录的功能，每一个文件的内部代码我会在接下来的几个博客里面详细的讲到。\n2.6.1 assets # 这个文件下面保存了YOLO历史上可以说最最最经典的两张图片了，这个是大家用来基础推理时候的图片，给大家测试用的。\n2.6.2 cfg（重点） # 这个文件下面保存了我们的模型配置文件，cfg目录是项目配置的集中地，其中包括：\ndatasets文件夹：包含数据集的配置文件，如数据路径、类别信息等（就是我们训练YOLO模型的时候需要一个数据集，这里面就保存部分数据集的yaml文件，如果我们训练的时候没有指定数据集则会自动下载其中的数据集文件，但是很容易失败！）。\nmodels文件夹：存放模型配置文件，定义了模型结构和训练参数等，这个是我们改进或者就基础版本的一个yaml文件配置的地方，截图如下:\nmodels文件夹中的每个.yaml文件代表了不同的YOLOv8模型配置，具体包括：\nyolov8.yaml: 这是YOLOv8模型的标准配置文件，定义了模型的基础架构和参数。\nyolov8-cls.yaml: 配置文件调整了YOLOv8模型，专门用于图像分类任务。\nyolov8-ghost.yaml: 应用Ghost模块的YOLOv8变体，旨在提高计算效率。\nyolov8-ghost-p2.yaml 和 yolov8-ghost-p6.yaml: 这些文件是针对特定大小输入的Ghost模型变体配置。\nyolov8-p2.yaml和 yolov8-p6.yaml: 针对不同处理级别（例如不同的输入分辨率或模型深度）的YOLOv8模型配置。\nyolov8-pose.yaml: 为姿态估计任务定制的YOLOv8模型配置。\nyolov8-pose-p6.yaml: 针对更大的输入分辨率或更复杂的模型架构姿态估计任务。\nyolov8-rtdetr.yaml: 可能表示实时检测和跟踪的YOLOv8模型变体。\nyolov8-seg.yaml 和 yolov8-seg-p6.yaml: 这些是为语义分割任务定制的YOLOv8模型配置。\n这些配置文件是模型训练和部署的核心，同时大家如果进行改进也是修改其中的对应文件来优化 网络结构。\ntrackers文件夹：用于追踪算法的配置。\n__init__.py文件：表明`cfg`是一个Python包。\ndefault.yaml：项目的默认配置文件，包含了被多个模块共享的通用配置项。\n这个文件就是配置训练的时候进行用的然后一些任务选择部分\n2.6.3 data # 在data/scripts文件夹中，包括了一系列脚本和Python文件：\n- download_weights.sh: 用来下载预训练权重的脚本。\n- get_coco.sh, get_coco128.sh, get_imagenet.sh: 用于下载COCO数据集完整版、128张图片版以及ImageNet数据集的脚本。\n在data文件夹中，包括：\nannotator.py: 用于数据注释的工具。\naugment.py: 数据增强相关的函数或工具。\nbase.py, build.py, converter.py: 包含数据处理的基础类或函数、构建数据集的脚本以及数据格式转换工具。\ndataset.py: 数据集加载和处理的相关功能。\nloaders.py: 定义加载数据的方法。\nutils.py: 各种数据处理相关的通用工具函数。\n2.6.4 engine # engine文件夹包含与模型训练、评估和推理有关的核心代码：\nexporter.py: 用于将训练好的模型导出到其他格式，例如ONNX或TensorRT。\nmodel.py: 包含模型定义，还包括模型初始化和加载的方法。\npredictor.py: 包含推理和预测的逻辑，如加载模型并对输入数据进行预测。\nresults.py: 用于存储和处理模型输出的结果。\ntrainer.py: 包含模型训练过程的逻辑。\ntuner.py: 用于模型超参数调优。\nvalidator.py: 包含模型验证的逻辑，如在验证集上评估模型性能。\n2.6.5 hub # hub文件夹通常用于处理与平台或服务集成相关的操作，包括：\nauth.py: 处理认证流程，如API密钥验证或OAuth流程。\nsession.py: 管理会话，包括创建和维护持久会话。\nutils.py: 包含一些通用工具函数，可能用于支持认证和会话管理功能。\n2.6.6 models(重点) # 这个目录下面是YOLO仓库包含的一些模型的方法实现，我们这里之说YOLO的，同时这里只是简单介绍，后面的博客针对于其中的任意一个都会进行单独的讲解。\n这个models/yolo目录中包含了YOLO模型的不同任务特定实现：\nclassify: 这个目录可能包含用于图像分类的YOLO模型。\ndetect: 包含用于物体检测的YOLO模型。\npose: 包含用于姿态估计任务的YOLO模型。\nsegment: 包含用于图像分割的YOLO模型，\n2.6.7 nn(重点) # 这个文件目录下的所有文件，就是定义我们模型中的一些组成构建，之后我们进行改进和优化，增加其它结构的时候都要在对应的文件下面进行改动。\nmodules文件夹:\n__init__.py: 表明此目录是Python包。\nblock.py: 包含定义神经网络中的基础块，如残差块或瓶颈块。\nconv.py: 包含卷积层相关的实现。\nhead.py: 定义网络的头部，用于预测。\ntransformer.py: 包含Transformer模型相关的实现。\nutils.py: 提供构建神经网络时可能用到的辅助函数。\n__init__.py: 同样标记这个目录为Python包。\nautobackend.py: 用于自动选择最优的计算后端。\ntasks.py: 定义了使用神经网络完成的不同任务的流程，例如分类、检测或分割，所有的流程基本上都定义在这里，定义模型前向传播都在这里。\n2.6.8 solutions # __init__.py: 标识这是一个Python包。\nai_gym.py: 与强化学习相关，例如在OpenAI Gym环境中训练模型的代码。\nheatmap.py: 用于生成和处理热图数据，这在物体检测和事件定位中很常见。\nobject_counter.py: 用于物体计数的脚本，包含从图像中检测和计数实例的逻辑。\n2.6.9 trackers # trackers文件夹包含了实现目标跟踪功能的脚本和模块：\n__init__.py: 指示该文件夹是一个Python包。\nbasetrack.py: 包含跟踪器的基础类或方法。\nbot_sort.py: 实现了SORT算法（Simple Online and Realtime Tracking）的版本。\nbyte_tracker.py: 是一个基于深度学习的跟踪器，使用字节为单位跟踪目标。\ntrack.py: 包含跟踪单个或多个目标的具体逻辑。\nREADME.md: 提供该目录内容和用法的说明。\n2.6.10 utils # 这个utils目录包含了多个Python脚本，每个脚本都有特定的功能：\ncallbacks.py: 包含在训练过程中被调用的回调函数。\nautobatch.py: 用于实现批处理优化，以提高训练或推理的效率。\nbenchmarks.py: 包含性能基准测试相关的函数。\nchecks.py: 用于项目中的各种检查，如参数验证或环境检查。\ndist.py: 涉及分布式计算相关的工具。\ndownloads.py: 包含下载数据或模型等资源的脚本。\nerrors.py: 定义错误处理相关的类和函数。\nfiles.py: 包含文件操作相关的工具函数。\ninstance.py: 包含实例化对象或模型的工具。\nloss.py: 定义损失函数。\nmetrics.py: 包含评估模型性能的指标计算函数。\nops.py: 包含自定义操作，如特殊的数学运算或数据转换。\npatches.py: 用于实现修改或补丁应用的工具。\nplotting.py: 包含数据可视化相关的绘图工具。\ntal.py: 一些损失函数的功能应用\ntorch_utils.py: 提供PyTorch相关的工具和辅助函数，包括GFLOPs的计算。\ntriton.py: 可能与NVIDIA Triton Inference Server集成相关。\ntuner.py: 包含模型或算法调优相关的工具。\n到这里重点的ultralytics文件目录下的所有功能都介绍完毕了，这里只是简单的介绍，后面的博客会详细的介绍一些重要的功能。\n2.7 同级目录下的文件 # 这个里是项目的根本配置和文档文件：\n.gitignore: Git配置文件，指定了Git版本控制要忽略的文件。\n.pre-commit-config.yaml: 预提交钩子的配置文件，用于在提交前自动执行代码质量检查。\nCITATION.cff: 提供了如何引用该项目的格式说明。\nCONTRIBUTING.md: 说明如何为项目贡献代码的指南。\nLICENSE: 包含了项目的许可证信息。\nMANIFEST.in: 列出了在构建和分发Python包时需要包含的文件。\nREADME.md 和 README.zh-CN.md: 项目的说明文件，分别为英文和中文版本。\nrequirements.txt: 列出了项目运行所需的Python依赖。\nsetup.cfg 和 setup.py: 包含了设置项目安装和分发的脚本。\n","date":"2024-10-03","externalUrl":null,"permalink":"/docs/hazaed/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e一、本文介绍 \n    \u003cdiv id=\"%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eHello，大家好这次给大家带来的不是改进，\u003cstrong\u003e是整个YOLOv8项目的分析\u003c/strong\u003e，\u003cstrong\u003e整个系列大概会更新7-10篇左右的文章\u003c/strong\u003e，从项目的目录到每一个功能代码的都会进行详细的讲解，同时YOLOv8改进系列也突破了三十篇文章，最后预计本专栏持续更新会在年底更新上百篇的改进教程， 所以大家如果没有订阅专栏可以提前订阅以下。下面开始进行YOLOv8逐行解析的第一篇——\u003cstrong\u003e项目目录构造分析\u003c/strong\u003e\u003c/p\u003e","title":"YOLOv8目录结构","type":"docs"},{"content":" 一、本文介绍 # 本文给大家带来的是YOLOv8项目的解读，之前给大家分析了YOLOv8的项目文件分析，这一篇文章给大家带来的是模型训练从我们的yaml文件定义到模型的定义部分的讲解，我们一般只知道如何去训练模型，和配置yaml文件，但是对于yaml文件是如何输入到模型里，模型如何将yaml文件解析出来的确是不知道的，本文的内容接上一篇的代码逐行解析(一) 项目目录分析，本文对于小白来说非常友好，非常推荐大家进行阅读，深度的了解模型的工作原理已经流程，下面我们从yaml文件来讲解。\n本文的讲解全部在代码的对应位置进行注释介绍非常详细，以下为部分内容的截图。\n二、yaml文件的定义 # 我们训练模型的第一步是需要配置yaml文件，我们的讲解第一步也从yaml文件来开始讲解，YOLOv8的yaml文件存放在我们的如下目录内\u0026rsquo;ultralytics/cfg/models/v8\u0026rsquo;，在其中我们可以定义各种模型配置的文件组合不同的模块，我们拿最基础的YOLOv8yaml文件来讲解一下。\n注释部分的内容我就不介绍了，我只介绍一下其中有用的部分，我已经在代码中对应的位置注释上了解释，大家可以看这样看起来也直观一些。\n# Ultralytics YOLO 🚀, AGPL-3.0 license # YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect # Parameters nc: 80 # 数据集的类别数，我们默认的数据COCO是80类别（YOLOv8提供的权重也是由此数据集训练出来的），有的读者喜欢修改nc此处其实不需要修改， # 模型会自动根据我们数据集的yaml文件获取此处的数量，同时我们8.1版本之前的ultralytics仓库打印两边的网络结构，唯一的区别就是nc的数量不一样（实际运行的是第二遍的网络结构）。 scales: # model compound scaling constants, i.e. \u0026#39;model=yolov8n.yaml\u0026#39; will call yolov8.yaml with scale \u0026#39;n\u0026#39; # 此处的含义大概就是如果我们在训练的指令时候使用model=yolov8.yaml 则对应的是v8n，如果使用model=yolov8s.yaml则对应的是v8s # 当然了大家如果不想使用上面的方式指定模型，我们只需要将下面想要使用的模型移到最前端即可，或者将其余不想用的注释掉都可以。 # [depth, width, max_channels] n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients, 28.8 GFLOPs m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients, 79.3 GFLOPs l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs # YOLOv8.0n backbone (主干部分的配置) backbone: # [from, repeats, module, args] # 这里需要多介绍一下，from, repeats, module, args # from 此处有三种可能的值分别是 -1、具体的数值、list存放数值。分别含义如下 (1)、-1的含义就是代表此层的输入就是上一层的输出， # (2)、如果是具体的某个数字比如4那么则代表本层的输入来自于模型的第四层， # (3)、有的层是list存放两个值也可能是多个值，则代表对应两个值的输出为本层的输入 # repeats 这个参数是为了C2f设置的其它的模块都用不到，代表着C2f当中Bottleneck重复的次数，比如当我们的模型用的是l的时候，那么repeats=3那么则代表C2f当中的Bottleneck串行3个。 # module 此处则代表模型的名称 # args 此处代表输入到对应模块的参数，此处和parse_model函数中的定义方法有关，对于C2f来说传入的参数-\u0026gt;第一个参数是上一个模型的输出通道数，第二个参数就是args的第一个参数，然后以此类推。 - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2 - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4 - [-1, 3, C2f, [128, True]] - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8 - [-1, 6, C2f, [256, True]] - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16 - [-1, 6, C2f, [512, True]] - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32 - [-1, 3, C2f, [1024, True]] - [-1, 1, SPPF, [1024, 5]] # 9 # YOLOv8.0n head head: - [-1, 1, nn.Upsample, [None, 2, \u0026#39;nearest\u0026#39;]] - [[-1, 6], 1, Concat, [1]] # cat backbone P4 - [-1, 3, C2f, [512]] # 12 - [-1, 1, nn.Upsample, [None, 2, \u0026#39;nearest\u0026#39;]] - [[-1, 4], 1, Concat, [1]] # cat backbone P3 - [-1, 3, C2f, [256]] # 15 (P3/8-small) - [-1, 1, Conv, [256, 3, 2]] - [[-1, 12], 1, Concat, [1]] # cat head P4 - [-1, 3, C2f, [512]] # 18 (P4/16-medium) - [-1, 1, Conv, [512, 3, 2]] - [[-1, 9], 1, Concat, [1]] # cat head P5 - [-1, 3, C2f, [1024]] # 21 (P5/32-large) - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5) 其中的Conv和C2f的结构我就不过多解释了，网上教程已经很多了，其中详细的结构在下图中都能够看到。\n三、yaml文件的输入 # 上面我们解释了yaml文件中的参数含义，然后提供了一个结构图（其中能够获取到每个模块的详细结构，该结构图来源于官方）。然后我们下一步介绍当定义好了一个ymal文件其是如何传入到模型的内部的，模型的开始在哪里。\n3.1 模型的定义 # 我们通过命令行的命令或者创建py文件运行模型之后，模型最开始的工作是模型的定义操作。模型存放于文件\u0026rsquo;ultralytics/engine/model.py\u0026rsquo;内部，首先需要通过\u0026rsquo;__init__\u0026lsquo;来定义模型的一些变量。\n此处我将模型的定义部分的代码解释了一下，大家有兴趣的可以和自己的文件对比着看。\nclass Model(nn.Module): import torch.nn as nn class Model(nn.Module): \u0026#34;\u0026#34;\u0026#34; 一个统一所有模型API的基类。 参数: model (str, Path): 要加载或创建的模型文件的路径。 task (Any, 可选): YOLO模型的任务类型。默认为None。 属性: predictor (Any): 预测器对象。 model (Any): 模型对象。 trainer (Any): 训练器对象。 task (str): 模型任务类型。 ckpt (Any): 如果从*.pt文件加载的模型，则为检查点对象。 cfg (str): 如果从*.yaml文件加载的模型，则为模型配置。 ckpt_path (str): 检查点文件路径。 overrides (dict): 训练器对象的覆盖。 metrics (Any): 用于度量的数据。 方法: __call__(source=None, stream=False, **kwargs): 预测方法的别名。 _new(cfg:str, verbose:bool=True) -\u0026gt; None: 初始化一个新模型，并从模型定义中推断任务类型。 _load(weights:str, task:str=\u0026#39;\u0026#39;) -\u0026gt; None: 初始化一个新模型，并从模型头中推断任务类型。 _check_is_pytorch_model() -\u0026gt; None: 如果模型不是PyTorch模型，则引发TypeError。 reset() -\u0026gt; None: 重置模型模块。 info(verbose:bool=False) -\u0026gt; None: 记录模型信息。 fuse() -\u0026gt; None: 为了更快的推断，融合模型。 predict(source=None, stream=False, **kwargs) -\u0026gt; List[ultralytics.engine.results.Results]: 使用YOLO模型进行预测。 返回: list(ultralytics.engine.results.Results): 预测结果。 \u0026#34;\u0026#34;\u0026#34; def __init__(self, model: Union[str, Path] = \u0026#34;yolov8n.pt\u0026#34;, task=None, verbose=False) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Initializes the YOLO model. Args: model (Union[str, Path], optional): Path or name of the model to load or create. Defaults to \u0026#39;yolov8n.pt\u0026#39;. task (Any, optional): Task type for the YOLO model. Defaults to None. verbose (bool, optional): Whether to enable verbose mode. \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; 此处为上面的解释 初始化 YOLO 模型。 参数: model (Union[str, Path], 可选): 要加载或创建的模型的路径或名称。默认为\u0026#39;yolov8n.pt\u0026#39;。 task (Any, 可选): YOLO 模型的任务类型。默认为 None。 verbose (bool, 可选): 是否启用详细模式。 \u0026#34;\u0026#34;\u0026#34; super().__init__() \u0026#34;\u0026#34;\u0026#34;此处就是读取我们的yaml文件的地方，callbacks.get_default_callbacks()会将我们的yaml文件进行解析然后将名称返回回来存放在self.callbacks中\u0026#34;\u0026#34;\u0026#34; self.callbacks = callbacks.get_default_callbacks() \u0026#34;\u0026#34;\u0026#34; 下面的部分就是一些模型的参数定义，我大概解释了一下，大家其实也不用太了解，一篇文章也介绍不了太多\u0026#34;\u0026#34;\u0026#34; self.predictor = None # 重用预测器 self.model = None # 模型对象 self.trainer = None # 训练器对象 self.ckpt = None # 如果从*.pt文件加载的检查点对象 self.cfg = None # 如果从*.yaml文件加载的模型配置 self.ckpt_path = None # 检查点文件路径 self.overrides = {} # 训练器对象的覆盖设置 self.metrics = None # 验证/训练指标 self.session = None # HUB 会话 self.task = task # 任务类型 self.model_name = model = str(model).strip() # 去除空格 # 检查是否为来自 https://hub.ultralytics.com 的 Ultralytics HUB 模型 if self.is_hub_model(model): # 从 HUB 获取模型 checks.check_requirements(\u0026#34;hub-sdk\u0026gt;0.0.2\u0026#34;) self.session = self._get_hub_session(model) model = self.session.model_file # 检查是否为 Triton 服务器模型 elif self.is_triton_model(model): self.model = model self.task = task return # 加载或创建新的 YOLO 模型 model = checks.check_model_file_from_stem(model) # 添加后缀，例如 yolov8n -\u0026gt; yolov8n.pt \u0026#34;\u0026#34;\u0026#34; 此处比较重要,如果我们没有指定模型的权重.pt那么模型会根据yaml文件创建一个新的模型，如果指定了权重那么模型这回加载pt文件中的模型\u0026#34;\u0026#34;\u0026#34; if Path(model).suffix in (\u0026#34;.yaml\u0026#34;, \u0026#34;.yml\u0026#34;): self._new(model, task=task, verbose=verbose) else: self._load(model, task=task) self.model_name = model # 返回的模型则保存在self.model_name中 3.2 模型的训练 # 我们上面讲完了模型的定义，然后模型就会根据你指定的参数来进行调用对应的函数，比如我这里指定的是detect，和train，如下图所示，然后模型就会根据指定的参数进行对应任务的训练。\n图片来源于文件\u0026rsquo;ultralytics/cfg/default.yaml\u0026rsquo; 截图。\n此处执行的是ultralytics/engine/model.py\u0026rsquo;文件中class Model(nn.Module):类别的def train(self, trainer=None, **kwargs):函数，具体的解释我已经在代码中标记了。\ndef train(self, trainer=None, **kwargs): \u0026#34;\u0026#34;\u0026#34; 在给定的数据集上训练模型。 参数: trainer (BaseTrainer, 可选): 自定义的训练器。 **kwargs (Any): 表示训练配置的任意数量的参数。 \u0026#34;\u0026#34;\u0026#34; self._check_is_pytorch_model() # 检查模型是否为 PyTorch 模型 if hasattr(self.session, \u0026#34;model\u0026#34;) and self.session.model.id: # Ultralytics HUB session with loaded model if any(kwargs): LOGGER.warning(\u0026#34;WARNING ⚠️ 使用 HUB 训练参数，忽略本地训练参数。\u0026#34;) kwargs = self.session.train_args # 覆盖 kwargs checks.check_pip_update_available() # 检查 pip 是否有更新 overrides = yaml_load(checks.check_yaml(kwargs[\u0026#34;cfg\u0026#34;])) if kwargs.get(\u0026#34;cfg\u0026#34;) else self.overrides custom = {\u0026#34;data\u0026#34;: DEFAULT_CFG_DICT[\u0026#34;data\u0026#34;] or TASK2DATA[self.task]} # 方法的默认设置 args = {**overrides, **custom, **kwargs, \u0026#34;mode\u0026#34;: \u0026#34;train\u0026#34;} # 最高优先级的参数在右侧 if args.get(\u0026#34;resume\u0026#34;): args[\u0026#34;resume\u0026#34;] = self.ckpt_path # 实例化或加载训练器 \u0026#34;\u0026#34;\u0026#34; 此处将一些参数加载到模型的内部\u0026#34;\u0026#34;\u0026#34; self.trainer = (trainer or self._smart_load(\u0026#34;trainer\u0026#34;))(overrides=args, _callbacks=self.callbacks) if not args.get(\u0026#34;resume\u0026#34;): # 仅在不续训的时候手动设置模型 # 获取模型并设置训练器 \u0026#34;\u0026#34;\u0026#34; 此处比较重要,为开始定义我们的对应任务的模型了比如我这里task设置的为Detect,那么此处会实例化DetectModel模型。 模型存放在ultralytics/nn/tasks.py内（就是我们修改模型时候的用到的那个task.py文件） 此处就会跳转到\u0026#39;ultralytics/nn/tasks.py\u0026#39;文化内的class DetectionModel(BaseModel):类中进行初始化和模型的定义工作 \u0026#34;\u0026#34;\u0026#34; self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml) self.model = self.trainer.model if SETTINGS[\u0026#34;hub\u0026#34;] is True and not self.session: # 如果开启了 HUB 并且没有 HUB 会话 try: # 创建一个 HUB 中的模型 self.session = self._get_hub_session(self.model_name) if self.session: self.session.create_model(args) # 检查模型是否创建成功 if not getattr(self.session.model, \u0026#34;id\u0026#34;, None): self.session = None except (PermissionError, ModuleNotFoundError): # 忽略 PermissionError 和 ModuleNotFoundError，表示 hub-sdk 未安装 pass # 将可选的 HUB 会话附加到训练器 self.trainer.hub_session = self.session # 进行模型训练 self.trainer.train() # 训练结束后更新模型和配置信息 if RANK in (-1, 0): ckpt = self.trainer.best if self.trainer.best.exists() else self.trainer.last self.model, _ = attempt_load_one_weight(ckpt) self.overrides = self.model.args self.metrics = getattr(self.trainer.validator, \u0026#34;metrics\u0026#34;, None) # TODO: DDP 模式下没有返回指标 return self.metrics 3.3 模型的网络结构打印 # 第三步比较重要的就是来到了\u0026rsquo;ultralytics/nn/tasks.py\u0026rsquo;（就是我们改进模型时候的那个文件）文化内的class DetectionModel(BaseModel):类中进行初始化和模型的定义工作。\n这里涉及到了模型的定义和校验工作（在模型的正式开始训练之前检测模型是否能够运行的工作！）。 class DetectionModel(BaseModel): \u0026#34;\u0026#34;\u0026#34;YOLOv8 目标检测模型。\u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg=\u0026#34;yolov8n.yaml\u0026#34;, ch=3, nc=None, verbose=True): # model, input channels, number of classes \u0026#34;\u0026#34;\u0026#34;使用给定的配置和参数初始化 YOLOv8 目标检测模型。\u0026#34;\u0026#34;\u0026#34; super().__init__() self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg) # cfg 字典 # 定义模型 ch = self.yaml[\u0026#34;ch\u0026#34;] = self.yaml.get(\u0026#34;ch\u0026#34;, ch) # 输入通道数 if nc and nc != self.yaml[\u0026#34;nc\u0026#34;]: LOGGER.info(f\u0026#34;覆盖 model.yaml nc={self.yaml[\u0026#39;nc\u0026#39;]} 为 nc={nc}\u0026#34;) self.yaml[\u0026#34;nc\u0026#34;] = nc # 覆盖 YAML 中的值 \u0026#34;\u0026#34;\u0026#34; 此处最为重要，涉及到了我们修改模型的配置的那个函数parse_model, 这里返回了我们的每一个模块的定义，也就是self.model保存了我们的ymal文件所有模块的实例化模型 self.save保存列表 | 也就是除了from部分为-1的部分比如from为4那么就将第四层的索引保存这里留着后面备用， \u0026#34;\u0026#34;\u0026#34; self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose) # 模型，保存列表 self.names = {i: f\u0026#34;{i}\u0026#34; for i in range(self.yaml[\u0026#34;nc\u0026#34;])} # 默认名称字典 self.inplace = self.yaml.get(\u0026#34;inplace\u0026#34;, True) # 构建步长 m = self.model[-1] # Detect() if isinstance(m, (Detect, Segment, Pose, Detect_AFPN4, Detect_AFPN3, Detect_ASFF, Detect_FRM, Detect_dyhead, CLLAHead, Detect_dyhead3, Detect_DySnakeConv, Segment_DySnakeConv, Segment_DBB, Detect_DBB, Pose_DBB, OBB, Detect_FASFF)): s = 640 # 2x 最小步长 m.inplace = self.inplace forward = lambda x: self.forward(x)[0] if isinstance(m, (Segment, Segment_DySnakeConv, Pose, Pose_DBB, Segment_DBB, OBB)) else self.forward(x) try: m.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, ch, s, s))]) # 在 CPU 上进行前向传播 except RuntimeError: try: self.model.to(torch.device(\u0026#39;cuda\u0026#39;)) m.stride = torch.tensor([s / x.shape[-2] for x in forward( torch.zeros(1, ch, s, s).to(torch.device(\u0026#39;cuda\u0026#39;)))]) # 在 CUDA 上进行前向传播 except RuntimeError as error: raise error self.stride = m.stride m.bias_init() # 仅运行一次 else: self.stride = torch.Tensor([32]) # 默认步长，例如 RTDETR # 初始化权重和偏置 initialize_weights(self) if verbose: # 此处为获取模型参数量和打印的地方。 self.info() LOGGER.info(\u0026#34;\u0026#34;) 3.4 parse_model的解析 # 这里涉及到yaml文件中模块的定义和，通道数放缩的地方，此处大家可以仔细看看比较重要涉及到模块的改动。\ndef parse_model(d, ch, verbose=True): # model_dict, input_channels(3) \u0026#34;\u0026#34;\u0026#34;解析 YOLO 模型.yaml 字典为 PyTorch 模型。\u0026#34;\u0026#34;\u0026#34; import ast # 参数设置 max_channels = float(\u0026#34;inf\u0026#34;) # 设置一个最大的通道数inf,防止后面的通道数有的超出了范围，没什么作用其实。 \u0026#34;\u0026#34;\u0026#34;下面一行代码比较重要，为获取我们yaml文件中的参数,nc=类别数（前面解释过了） act=激活函数， scales=模型的大小\u0026#34;\u0026#34;\u0026#34; nc, act, scales = (d.get(x) for x in (\u0026#34;nc\u0026#34;, \u0026#34;activation\u0026#34;, \u0026#34;scales\u0026#34;)) \u0026#34;\u0026#34;\u0026#34;此处为获取模型的通道数放缩比例假如 n: [0.33, 0.25, 1024] # YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;那么此处对应的就是 0.33 , 0.25, 1024\u0026#34;\u0026#34;\u0026#34; depth, width, kpt_shape = (d.get(x, 1.0) for x in (\u0026#34;depth_multiple\u0026#34;, \u0026#34;width_multiple\u0026#34;, \u0026#34;kpt_shape\u0026#34;)) \u0026#34;\u0026#34;\u0026#34;下面这个判断主要的功能就是我们指定yaml文件的时候如果不指定n或者其它模型尺度则默认用n然后提出一个警告，细心的读者应该会遇到过这个警告，群里也有人问过\u0026#34;\u0026#34;\u0026#34; if scales: scale = d.get(\u0026#34;scale\u0026#34;) if not scale: scale = tuple(scales.keys())[0] LOGGER.warning(f\u0026#34;WARNING ⚠️ 没有传递模型比例。假定 scale=\u0026#39;{scale}\u0026#39;。\u0026#34;) depth, width, max_channels = scales[scale] if act: Conv.default_act = eval(act) # 重新定义默认激活函数，例如 Conv.default_act = nn.SiLU() if verbose: LOGGER.info(f\u0026#34;{colorstr(\u0026#39;activation:\u0026#39;)} {act}\u0026#34;) # 打印 if verbose: LOGGER.info(f\u0026#34;\\n{\u0026#39;\u0026#39;:\u0026gt;3}{\u0026#39;from\u0026#39;:\u0026gt;20}{\u0026#39;n\u0026#39;:\u0026gt;3}{\u0026#39;params\u0026#39;:\u0026gt;10} {\u0026#39;module\u0026#39;:\u0026lt;45}{\u0026#39;arguments\u0026#39;:\u0026lt;30}\u0026#34;) ch = [ch] # 存放第一个输入的通道数,这个ch后面会存放所有层的通道数，第一层为通道数是ch=3也就是对应我们一张图片的RGB图片的三基色三个通道，分别对应红绿蓝！ layers, save, c2 = [], [], ch[-1] # 提前定义一些之后存放的容器分别为，模型层，保存列表，输出通道数 \u0026#34;\u0026#34;\u0026#34;下面开始正式解析模型的yaml文件然后进行定义的操作用for训练便利yaml文件\u0026#34;\u0026#34;\u0026#34; for i, (f, n, m, args) in enumerate(d[\u0026#34;backbone\u0026#34;] + d[\u0026#34;head\u0026#34;]): # from, number, module, args m = getattr(torch.nn, m[3:]) if \u0026#34;nn.\u0026#34; in m else globals()[m] # 获取模块 for j, a in enumerate(args): if isinstance(a, str): with contextlib.suppress(ValueError): args[j] = locals()[a] if a in locals() else ast.literal_eval(a) \u0026#34;\u0026#34;\u0026#34; 此处为repeat那个参数的放缩操作,不过多解释了,最小的n是1（就是是说你yaml文件里定义的是3，然后和放缩系数相乘然后和1比那个小取那个）\u0026#34;\u0026#34;\u0026#34; n = n_ = max(round(n * depth), 1) if n \u0026gt; 1 else n \u0026#34;\u0026#34;\u0026#34;下面是一些具体模块的定义操作了\u0026#34;\u0026#34;\u0026#34; if m in (Classify, Conv, ConvTranspose, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, DWConv, Focus, BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, nn.ConvTranspose2d, DWConvTranspose2d, C3x, RepC3): c1, c2 = ch[f], args[0] if c2 != nc: # 如果 c2 不等于类别数（即 Classify() 输出） \u0026#34;\u0026#34;\u0026#34; 绝大多数情况下都不等，我们放缩通道数，也就是为什么不同大小的模型参数量不一致的地方因为参数量主要由通道数决定，GFLOPs主要有图像的宽和高决定\u0026#34;\u0026#34;\u0026#34; c2 = make_divisible(min(c2, max_channels) * width, 8) if m is C2fAttn: args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8) # 嵌入通道数 args[2] = int( max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] \u0026gt; 1 else args[2] ) # 头部数量 \u0026#34;\u0026#34;\u0026#34;此处需要解释一下，大家需要仔细注意此处\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; 这个args就是传入到我们模型的参数,C1就是上一层的或者指定层的输出的通道数，C2就是本层的输出通道数， *args[1:]就是其它的一些参数比如卷积核步长什么的\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; 此处和注意力机制不同的是，为什么注意力机制不在此处添加因为注意力机制不改变模型的维度，所以一般只需要指定一个输入通道数就行， 所以这也是为什么我们在后面定义注意力需要额外添加代码的原因有兴趣的读者可以对比一下\u0026#34;\u0026#34;\u0026#34; args = [c1, c2, *args[1:]] \u0026#34;\u0026#34;\u0026#34; 此处就是涉及的上面求出的实际的n然后插入的参数列表中去，然后准备在最下面进行传参\u0026#34;\u0026#34;\u0026#34; if m in (BottleneckCSP, C1, C2, C2f, C2fAttn, C3, C3TR, C3Ghost, C3x, RepC3): args.insert(2, n) # 重复次数 n = 1 \u0026#34;\u0026#34;\u0026#34;这些都是一些具体的模块定义的方法，不多解释了\u0026#34;\u0026#34;\u0026#34; elif m is AIFI: args = [ch[f], *args] elif m in (HGStem, HGBlock): c1, cm, c2 = ch[f], args[0], args[1] args = [c1, cm, c2, *args[2:]] if m is HGBlock: args.insert(4, n) # 重复次数 n = 1 elif m is ResNetLayer: c2 = args[1] if args[3] else args[1] * 4 elif m is nn.BatchNorm2d: args = [ch[f]] elif m is Concat: c2 = sum(ch[x] for x in f) elif m in (Detect, WorldDetect, Segment, Pose, OBB, ImagePoolingAttn): args.append([ch[x] for x in f]) if m is Segment: args[2] = make_divisible(min(args[2], max_channels) * width, 8) elif m is RTDETRDecoder: # 特殊情况，channels 参数必须在索引 1 中传递 args.insert(1, [ch[x] for x in f]) else: c2 = ch[f] \u0026#34;\u0026#34;\u0026#34;此处就是模型的正式定义和传参的操作\u0026#34;\u0026#34;\u0026#34; m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n \u0026gt; 1 else m(*args) # 模块 t = str(m)[8:-2].replace(\u0026#34;__main__.\u0026#34;, \u0026#34;\u0026#34;) # 模块类型 m.np = sum(x.numel() for x in m_.parameters()) # 参数数量 m_.i, m_.f, m_.type = i, f, t # 附加索引，\u0026#39;from\u0026#39; 索引，类型 if verbose: LOGGER.info(f\u0026#34;{i:\u0026gt;3}{str(f):\u0026gt;20}{n_:\u0026gt;3}{m.np:10.0f} {t:\u0026lt;45}{str(args):\u0026lt;30}\u0026#34;) # 打印 \u0026#34;\u0026#34;\u0026#34;此处就是保存一些索引通道数涉及到from的部分，此处文字很难解释的清楚有兴趣可以自己debug看一下就明白了\u0026#34;\u0026#34;\u0026#34; save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1) # 添加到保存列表 layers.append(m_) if i == 0: ch = [] ch.append(c2) return nn.Sequential(*layers), sorted(save) 四、模型的结构打印 # 经过上面的分析之后，我们就会打印了模型的结构，图片如下所示，然后到此本篇文章的分析就到这里了，剩下的下一篇文章讲解。\n（需要注意的是上面的讲解整体是按照顺序但是是以递归的形式介绍，比如3.2是3.1当中的某一行代码的功能而不是结束之后才允许的3.2，而是3.1运行的过程中运行了3.2。）\n","date":"2024-10-03","externalUrl":null,"permalink":"/docs/rcnn/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e一、本文介绍 \n    \u003cdiv id=\"%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e本文给大家带来的是\u003cstrong\u003eYOLOv8项目的解读\u003c/strong\u003e，之前给大家分析了YOLOv8的项目文件分析，这一篇文章给大家带来的是模型训练从我们的yaml文件定义到模型的定义部分的讲解，我们一般只知道如何去训练模型，和配置yaml文件，但是对于yaml文件是如何输入到模型里，模型如何将yaml文件解析出来的确是不知道的，本文的内容接上一篇的代码逐行解析(一) 项目目录分析，本文对于小白来说非常友好，非常推荐大家进行阅读，深度的了解模型的工作原理已经流程，下面我们从yaml文件来讲解。\u003c/p\u003e","title":"YOLOv8文件分析","type":"docs"},{"content":" 说明文档 # 这篇文档主要介绍《基于YOLOv8的农田病虫害检测与分析》的代码实现部分，整篇论文的目的主要是改进YOLOv8的网络结构，使其在检测病虫害的精度和实时性上有所提升。接下来，我将介绍如何从零开始搭建起本项目。\n安装Python # 到python的官方网站：https://www.python.org/下载，安装\n安装完成后，在命令行窗口运行：python，查看安装的结果，如下图：\n至此，Python安装完成，接下来还需要安装anaconda，这是一个python虚拟环境，特别适合管理python的环境。\n安装anaconda # 到anaconda的官方网站：https://www.anaconda.com/download/success下载，并安装：\n安装成功后，会在开始菜单出现如下图所示：\nanaconda安装完成，接下来安装pycharm，主要用来编写代码。\n安装Pycharm # 学生可以申请教育版\n支持，所有的软件安装完成。\nYOLOv8目录结构介绍 # 首先介绍整个项目的目录：\n和原来的YOLOv8相比，根目录新增一些训练的脚本和测试的脚本，比如train.py和Detect.py，当然也可以直接通过命令行的方式来实现，两者效果都是一样的。\n重点是ultralytics/nn目录，所有的改进模块都是在这里进行，在这里我新建了一个Addmodules的目录，里面是改进的各种模块，包括主干网络，颈部网络和检测头的改进。\n需要修改的部分我都已经作了修改，不用再做其他的改动\n还有一个重要的目录：ultralytics/cfg/models/Add，这里面放的都是yaml文件，其中改进的yaml文件都已经写好，不需要改动。\n以下是一个yaml文件的示例，其它的都是类似的结构，只是参数不同：\n安装项目的环境（非常重要） # 环境配置非常重要，我当时配环境换了一周左右的时间，中间经历了各种报错，软件包不兼容的问题和显卡驱动匹配的问题，总之就是不好搞。为了方面复现工作，我已经把anaconda的环境导出为environment.yml，位于项目的根目录里面，创建虚拟环境的时候直接使用就可以\nanaconda虚拟环境 # 再anaconda prompt终端输入conda env create -f environment.yml，就可以根据environment.yml文件创建虚拟环境，创建好后，通过conda env list查看环境是否存在，如下图所示就表明创建成功：\n如果安装的时候出现torch相关的错误，大概率是你的显卡驱动和这里面的torch包版本不匹配，这个问题需要自行修改即可，网上关于这方面的资料很多。\n使用虚拟环境 # 虚拟环境创建完成之后，就可以在pycharm中使用，点击右下角，切换conda环境，选择刚才创建的虚拟环境。如果到了这一步还没有报错的话，恭喜你，已经完成了80%的工作。\n运行Detect.py脚本，测试检测效果，如果没有报错，接下来就是训练模型。\n训练脚本train.py # 找到根目录的train.py文件，注释已经写的很清楚，如下图：\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) from ultralytics import YOLO if __name__ == \u0026#39;__main__\u0026#39;: model = YOLO(\u0026#39;yolov8-HSFPN.yaml\u0026#39;) # model.load(\u0026#39;yolov8n.pt\u0026#39;) # 是否加载预训练权重,科研不建议大家加载否则很难提升精度 model.train(data=r\u0026#39;D:/Downloads/YOLOv8/datasets/data.yaml\u0026#39;, # 如果大家任务是其它的\u0026#39;ultralytics/cfg/default.yaml\u0026#39;找到这里修改task可以改成detect, segment, classify, pose cache=False, imgsz=640, epochs=150, single_cls=False, # 是否是单类别检测 batch=4, close_mosaic=10, workers=0, device=\u0026#39;0\u0026#39;, optimizer=\u0026#39;SGD\u0026#39;, # using SGD # resume=\u0026#39;runs/train/exp21/weights/last.pt\u0026#39;, # 如过想续训就设置last.pt的地址 amp=True, # 如果出现训练损失为Nan可以关闭amp project=\u0026#39;runs/train\u0026#39;, name=\u0026#39;exp\u0026#39;, ) model = YOLO(\u0026lsquo;yolov8-HSFPN.yaml\u0026rsquo;)，把里面的yaml文件换成自己的yaml文件，我这里用的是yolov8-HSFPN.yaml，data=r\u0026rsquo;D:/Downloads/YOLOv8/datasets/data.yaml，同理，换成自己数据集的yaml文件，我这里的数据集是yolo格式。其它的参数可以按照自己的任务自行调整。\n还有一个检测的脚本，Detect.py:\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) from ultralytics import YOLO if __name__ == \u0026#39;__main__\u0026#39;: model = YOLO(\u0026#39;D:/Downloads/YOLOv8/result/result_8_HSFPN/train/exp/weights/best.pt\u0026#39;) # select your model.pt path model.predict(source=\u0026#39;D:/Downloads/YOLOv8/ultralytics/assets\u0026#39;, imgsz=640, project=\u0026#39;runs/detect\u0026#39;, name=\u0026#39;exp\u0026#39;, save=True, ) 同理，把best.pt换成你自己训练好的模型，source里面输入检测图片的路径，运行该脚本就可以开始检测，结果保存在runs/detect目录。\n开始训练 # 准备好数据集，最好是yolo格式的，我的数据集项目里自带了，不需要重新下载：\ndatasets目录里面就是我的数据集：有train，test，valid三个目录，分别存放训练集，测试集和验证集的图像和标签：\n准备这些之后，运行train.py文件，开始训练。如果报错的话，请自行上网查找，无非就是找不到数据集，某个包的版本不对，或者是GPU用不了，只能用CPU。\n训练结果 # 训练结果会保存在runs/train目录下，exp1,exp2,exp3的顺序，表示每一次的训练结果。\n上图就是训练完成后目录的结构，weights目录里面就是我们需要的模型：best.pts是效果最好的，最后也是需要这个，last.pt是最后一次的训练结果。\n总结 # 整个项目的改进工作我已经做好，复现的话只需装好对应的环境，修改train.py的参数，运行train.py就可以开始训练；修改Detect.py的参数，就可以检测。目前项目只针对检测任务，对于分割和分类没有做改进。\n经验之谈 # （1）以下为两个重要库的版本，必须对应下载，否则会报错\npython == 3.9.7 pytorch == 1.12.1 timm == 0.9.12 # 此安装包必须要 mmcv-full == 1.6.2 # 不安装此包部分关于dyhead的代码运行不了以及Gold-YOLO\n（2）mmcv-full会安装失败是因为自身系统的编译工具有问题，也有可能是环境之间安装的有冲突\n推荐大家离线安装的形式,下面的地址中大家可以找找自己的版本,下载到本地进行安装。 https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html https://download.openmmlab.com/mmcv/dist/index.html （3）basicsr安装失败原因,通过pip install basicsr 下载如果失败,大家可以去百度搜一下如何换下载镜像源就可以修复\n针对一些报错的解决办法在这里说一下 # (1)训练过程中loss出现Nan值. 可以尝试关闭AMP混合精度训练.\n(2)多卡训练问题,修改模型以后不能支持多卡训练可以尝试下面的两行命令行操作，两个是不同的操作，是代表不同的版本现尝试第一个不行用第二个\npython -m torch.distributed.run --nproc_per_node 2 train.py python -m torch.distributed.launch --nproc_per_node 2 train.py (3) 针对运行过程中的一些报错解决 1.如果训练的过程中验证报错了(主要是一些形状不匹配的错误这是因为验证集的一些特殊图片导致) 找到ultralytics/models/yolo/detect/train.py的DetectionTrainer class中的build_dataset函数中的rect=mode == \u0026lsquo;val\u0026rsquo;改为rect=False\n2.推理的时候运行detect.py文件报了形状不匹配的错误 找到ultralytics/engine/predictor.py找到函数def pre_transform(self, im),在LetterBox中的auto改为False 3.训练的过程中报错类型不匹配的问题 找到\u0026#39;ultralytics/engine/validator.py\u0026#39;文件找到 \u0026#39;class BaseValidator:\u0026#39; 然后在其\u0026#39;__call__\u0026#39;中 self.args.half = self.device.type != \u0026#39;cpu\u0026#39; # force FP16 val during training的一行代码下面加上self.args.half = False (4) 针对yaml文件中的nc修改 不用修改，模型会自动根据你数据集的配置文件获取。 这也是模型打印两次的区别，第一次打印出来的就是你选择模型的yaml文件结构，第二次打印的就是替换了你数据集的yaml文件，模型使用的是第二种。\n(5) 针对环境的问题 环境的问题每个人遇见的都不一样，可自行上网查找。\n","date":"2024-10-03","externalUrl":null,"permalink":"/docs/installation/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e说明文档 \n    \u003cdiv id=\"%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e这篇文档主要介绍《基于YOLOv8的农田病虫害检测与分析》的代码实现部分，整篇论文的目的主要是改进YOLOv8的网络结构，使其在检测病虫害的精度和实时性上有所提升。接下来，我将介绍如何从零开始搭建起本项目。\u003c/p\u003e","title":"安装文档","type":"docs"},{"content":" 恒源云 # 为当涉及到深度学习的训练任务时，GPU的计算能力是不可或缺的。相对于传统的中央处理器（CPU），图形处理器（GPU）具有更强大的并行计算能力，能够显著加速深度学习模型的训练过程。深度学习算法通常涉及大量的矩阵运算和张量操作，而GPU的并行计算架构使得它们能够高效地执行这些计算，从而加速模型训练的速度。\n恒源云是一个经济高效的云计算平台，您可以通过恒源云的控制台或者命令行界面来管理实例、上传和下载数据、执行训练任务等。恒源云还提供了高度可定制的实例规格，您可以根据自己的需求选择适合的实例类型和配置，以最大程度地优化性能和成本。\n另一个恒源云的优势是其经济实惠的价格。相对于购买和维护专门的GPU设备，利用恒源云进行云端模型训练可以大大节省成本。恒源云提供了多种付费模式，包括按需付费和预付费套餐，使您能够根据自己的预算和需求进行灵活选择。\n上传数据集 # 在恒源云中我们需要通过终端来上传数据集文件，当在本地处理好了数据集文件以后，我们将其解压缩成zip文件的格式当然tar压缩包等格式的都可以。 这里推荐大家用OSS命令上传数据集,可以支持大规模的数据上传。\n在利用OSS进行上传之前我们需要下载一个文件，下载方式如下。\n完成之后，我们点击下载好的文件，会弹出命令行。\n在这里我们可以输入指令,我们先来输入version来检验下我们是否安装成功。\n当我们安装成功之后，我们先远程登录我们的账号和密码，输入Login\nlogin 当我们登录成功之后,我们就远程登录了我们的恒源云账号和密码,我们就可以在我们的账号下面建立存储我们数据的文件了。\n按照下图操作即可。 当我们上传好一个文件之后,该文件就保存到我们的系统内了,我们可以随时在该终端页面下载该数据到我们后面步骤中创建的任何实例当中，利用如下命令\n(PS:最后一步需要我们经过下面的\u0026rsquo;利用云端训练YOLOv8模型\u0026rsquo;之后才可以进行，在我们创建完实例之后进行的操作步骤)\n利用云端训练YOLOv8模型 # 首先进入恒源云的官方网站\n恒源云官方网站\n然后进行注册和登录操作此步骤省略\n当我们注册和登录之后会进到控制台界面,然后点击创建实例进入到如下界面。\n在其中根据你的需求选择你的GPU型号,\n之后在同页面的最下面有一个实例镜像，可以在其中的下拉滚动条中选择你需要的PyTorch、TensorFlow或者其它框架的版本\n然后之后我们创建实例即可。\n首先开始时需要创建一会,然后才可以进行操作，等待一会创建成功后就会变成如下图的状态情况。\n我们按照图片的操作点击其中的\u0026quot;JupyterLab\u0026quot; 然后会弹出新的网页如下图。\n在其中hy-tmp是一个存放我们文件的文件夹,我们点击进去点击图片上的上传本地文件操作即可上传你的模型文件。终端就是一个输入命令的地方，我们点击终端命令，如下图所示。\n我们初始的时候是在系统的根目录下面,我们进行模型训练等操作进入hy-tmp目录也就是你上传文件的目录下面。\n我们利用cd 命令进入hy-tmp目录\ncd hy-tmp 进入其中以后，上传我们的文件。\n可以看到我把YOLOv8的官方下载的压缩包上传了进去，其为zip格式的压缩包。\n此时在命令行输入命令解压缩该文件\n输入unzip 文件名.zip解压文件\nunzip 文件名.zip cd到该文件目录\ncd 文件名 输入ll 看文件目录下的结构 ll 这里我们演示的是利用YOLOv8进行目标检测时候的训练流程进行演示,我们进入ultralytics\\cfg文件目录利用cd进入\ncd ultralytics\\cfg 同理我们输入ll看该文件下的目录结构\n可以看到其中有一个default.yaml文件,该文件就是我们进行训练模型的文件,我们可以在左侧的目录下看该文件的代码。 当然其中的配置,我就不在这里描述了,如果有需要可以看我的YOLOv8详细训练教程里面有具体的配置以及教程。当我们配置好了数据集以及选择的模型之后就可以在官方的模型基础上进行训练了de。 此时我们需要退到ultralytics-main的目录下面执行下面的文件就可以进行训练了。\nyolo task=detect mode=train model=datasets/yolo8n.yaml data=datasets/data.yaml epochs=100 batch=64 device=0 single_cls=True pretrained=yolov8n.pt PS：在我们的系统中python解释器已经默认帮我们配置好了,如果你想要执行一个py格式文件，我们只需要输入python 文件名.py文件即可\npython 文件名.py 到此本教程就结束,希望对你有所帮助。大家如有任何问题可以在评论区进行提问。 ","date":"2024-10-03","externalUrl":null,"permalink":"/docs/fast-rcnn/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e恒源云 \n    \u003cdiv id=\"%E6%81%92%E6%BA%90%E4%BA%91\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E6%81%92%E6%BA%90%E4%BA%91\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e为当涉及到深度学习的训练任务时，GPU的计算能力是不可或缺的。相对于传统的中央处理器（CPU），图形处理器（GPU）具有更强大的并行计算能力，能够显著加速深度学习模型的训练过程。深度学习算法通常涉及大量的矩阵运算和张量操作，而GPU的并行计算架构使得它们能够高效地执行这些计算，从而加速模型训练的速度。\u003c/p\u003e","title":"恒源云","type":"docs"},{"content":" 一、简介 # 这篇博客，主要给大家讲解我们在训练yolov8时生成的结果文件中各个图片及其中指标的含义，帮助大家更深入的理解，以及我们在评估模型时和发表论文时主要关注的参数有那些。本文通过举例训练过程中的某一时间的结果来帮助大家理解，大家阅读过程中如有任何问题可以在评论区提问出来，我会帮助大家解答。首先我们来看一个在一次训练完成之后都能生成多少个文件如下图所示，下面的文章讲解都会围绕这个结果文件来介绍。\n二、评估用的数据集 # 上面的训练结果，是根据一个检测飞机的数据集训练得来，其中只有个标签就是飞机，对于这种单标签的数据集，其实我们可以将其理解为一个二分类任务，\n一种情况-\u0026gt;检测为飞机，另一种情况-\u0026gt;不是飞机。\n三、结果分析 # 我们可以从结果文件中看到其中共有文件24个，后12张图片是根据我们训练过程中的一些检测结果图片，用于我们可以观察检测结果，有哪些被检测出来了，那些没有被检测出来，其不作为指标评估的文件。 Weights文件夹 # 我们先从第一个weights文件夹来分析，其中有两个文件，分别是best.pt、last.pt,其分别为训练过程中的损失最低的结果和模型训练的最后一次结果保存的模型。\nargs.yaml # 第二个文件是args.yaml文件,其中主要保存一些我们训练时指定的参数，内容如下所示。\n混淆矩阵(ConfusionMatrix) # 第三个文件就是混淆矩阵，大家都应该听过这个名字，其是一种用于评估分类模型性能的表格形式。它以实际类别（真实值）和模型预测类别为基础，将样本分类结果进行统计和汇总。\n对于二分类问题，混淆矩阵通常是一个2×2的矩阵，包括真阳性（True Positive, TP）、真阴性（True Negative, TN）、假阳性（False Positive, FP）和假阴性（False Negative, FN）四个元素。\nTrue_Label = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1 ,0, 1, 0 , 1 , 0, 0 , 1]Predict_Label = [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1 ,0 , 0 , 1 , 0, 0 , 1, 0] 我们来分析这个图，其每个格子代表的含义我在图片上标注了出来**,下面我们来拿一个例子来帮助大家来理解这个混淆矩阵。**\n假设我们的数据集预测为飞机标记为数字0、预测不为飞机标记为1，现在假设我们在模型的训练的某一批次种预测了20次其真实结果和预测结果如下所示。 其中True_Label代表真实的标签，Predict_Label代表我们用模型预测的标签。\n那么我们可以进行对比产生如下分析\n6个样本的真实标签和预测标签都是0（真阴性，True Negative）。 1个样本的真实标签是0，但预测标签是1（假阳性，False Positive）。 8个样本的真实标签是1，但预测标签是0（假阴性，False Negative）。 5个样本的真实标签和预测标签都是1（真阳性，True Positive）。 下面根据我们的分析结果，我们就能够画出这个预测的混淆矩阵，\n由此我们就能得到那一批次的混淆矩阵，我们的最终结果生成的混淆矩阵可以理解为多个混淆矩阵的统计结果。 混淆矩阵归一化(Confusion Matrix Normal) # 这个混淆矩阵的归一化，就是对混淆矩阵做了一个归一化处理，对混淆矩阵进行归一化可以将每个单元格的值除以该类别实际样本数，从而得到表示分类准确率的百分比。这种标准化使得我们可以直观地比较类别间的分类准确率，并识别出模型在哪些类别上表现较好或较差。\n我们可以看到是对于列进行了归一化处理，0.9 + 0.1 = 1，1 + 0 = 1。 计算mAP、Precision、Recall # 在讲解其它的图片之前我们需要来计算三个比较重要的参数，这是其它图片的基础，这里的计算还是利用上面的某一批次举例的分析结果。\n精确度（Precision）：预测为正的样本中有多少是正确的，Precision = TP / (TP + FP) = 5 / (5 + 1) = 5/6 ≈ 0.833\n召回率（Recall）：真实为正的样本中有多少被正确预测为正，Recall = TP / (TP + FN) = 5 / (5 + 8) ≈ 0.385\nF1值（F1-Score）：**综合考虑精确度和召回率的指标，**F1 = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.833 * 0.385) / (0.833 + 0.385) ≈ 0.526\n准确度（Accuracy）：**所有样本中模型正确预测的比例，**Accuracy = (TP + TN) / (TP + TN + FP + FN) = (5 + 6) / (5 + 6 + 1 + 8) ≈ 0.565\n平均精确度（Average Precision, AP）：**用于计算不同类别的平均精确度，对于二分类问题，AP等于精确度。**AP = Precision = 0.833\n平均精确度（Mean Average Precision, mAP）：多类别问题的平均精确度，对于二分类问题，mAP等于AP（精确度），所以mAP = AP = 0.833\n这里需要讲解的主要是AP和MAP如果是多分类的问题，AP和mAP怎么计算，首先我们要知道AP的全称就是Average Precision，平均精度所以我们AP的计算公式如下？\nmAP就是Mean Average Precision，计算如下，计算每一个没别的AP进行求平均值处理就是mAP。\nF1_Curve # F1_Curve这个文件，我们点击去的图片的标题是F1-Confidence Curve它显示了在不同分类阈值下的F1值变化情况。\n我们可以这么理解，先看它的横纵坐标，横坐标是置信度，纵坐标是F1-Score，F1-Score在前面我们以及讲解过了，那什么是置信度？\n**置信度(Confidence)-\u0026gt;**在我们模型的识别过程中会有一个概率，就是模型判定一个物体并不是百分百判定它是属于某一个分类，它会给予它以个概率，Confidence就是我们设置一个阈值，如果超过这个概率那么就确定为某一分类，假如我模型判定一个物体由0.7的概率属于飞机，此时我们设置的阈值如果为0.7以下那么模型就会输出该物体为飞机，如果我们设置的阈值大于0.7那么模型就不会输出该物体为飞机。\nF1-Confidence Curve就是随着F1-Score随着Confience的逐渐增高而变化的一个曲线。\nLabels # Labels图片代表每个检测到的目标的类别和边界框信息。每个目标都由一个矩形边界框和一个类别标签表示，我们逆时针来看这个图片！！！\n目标类别：该像素点所检测到的目标类别，例如飞机等。 目标位置：该像素点所检测到的目标在图像中的位置，即该像素点在图像中的坐标。 目标大小：该像素点所检测到的目标的大小，即该像素点所覆盖的区域的大小。 其他信息：例如目标的旋转角度等其他相关信息。 labels_correlogram # labels_correlogram是一个在**机器学习领域中使用的术语，**它指的是一种图形，用于显示目标检测算法在训练过程中预测标签之间的相关性。\n具体来说，labels_correlogram是一张颜色矩阵图，它展示了训练集数据标签之间的相关性。它可以帮助我们理解目标检测算法在训练过程中的行为和表现，以及预测标签之间的相互影响。\n通过观察labels_correlogram，我们可以了解到目标检测算法在不同类别之间的区分能力，以及对于不同类别的预测精度。此外，我们还可以通过比较不同算法或不同数据集labels_correlogram，来评估算法的性能和数据集的质量。\n总之，labels_correlogram是一种有用的工具，可以帮助我们更好地理解目标检测算法在训练过程中的行为和表现，以及评估算法的性能和数据集的质量。\nP_curve # 这个图的分析和F1_Curve一样，不同的是关于的是Precision和Confidence之间的关系，可以看出我们随着置信度的越来越高检测的准确率按理来说是越来越高的。 R_curve # 这个图的分析和F1_Curve一样，不同的是关于的是Recall和Confidence之间的关系，可以看出我们随着置信度的越来越高召回率的准确率按理来说是越来越低的。 PR_curve # 它显示了在不同分类阈值下模型的精确度（Precision）和召回率（Recall）之间的关系。\nPR曲线越靠近坐标轴的右上角，模型性能越好，越能够正确识别正样本，正确分类正样本的Precision值越高，而靠近右侧则说明模型对正样本的识别能力较差，即召回能力较差。\nPR曲线的特点是随着分类阈值的变化，精确度和召回率会有相应的改变。通常情况下，当分类模型能够同时保持较高的精确度和较高的召回率时，PR曲线处于较高的位置。当模型偏向于高精确度或高召回率时，曲线则相应地向低精确度或低召回率的方向移动。\nPR曲线可以帮助我们评估模型在不同阈值下的性能，并选择适当的阈值来平衡精确度和召回率。对于模型比较或选择，我们可以通过比较PR曲线下方的面积（称为平均精确度均值，Average Precision, AP）来进行定量评估。AP值越大，模型的性能越好。\n总结：PR曲线是一种展示分类模型精确度和召回率之间关系的可视化工具，通过绘制精确度-召回率曲线，我们可以评估和比较模型在不同分类阈值下的性能，并计算平均精确度均值（AP）来定量衡量模型的好坏。\nresults.csv # results.csv记录了一些我们训练过程中的参数信息，包括损失和学习率等，这里没有什么需要理解大家可以看一看，我们后面的results图片就是根据这个文件绘画出来的。\nresults # 这个图片就是生成结果的最后一个了，我们可以看出其中标注了许多小的图片包括训练过程在的各种损失，我们主要看的其实就是后面的四幅图mAP50、mAP50-95、metrics/precision、metrics/recall四张图片。 mAP50：mAP是mean Average Precision的缩写，表示在多个类别上的平均精度。mAP50表示在50%的IoU阈值下的mAP值。 mAP50-95：这是一个更严格的评价指标，它计算了在50-95%的IoU阈值范围内的mAP值，然后取平均。这能够更准确地评估模型在不同IoU阈值下的性能。 metrics/precision：精度（Precision）是评估模型预测正确的正样本的比例。在目标检测中，如果模型预测的边界框与真实的边界框重合，则认为预测正确。 metrics/recall：召回率（Recall）是评估模型能够找出所有真实正样本的比例。在目标检测中，如果真实的边界框与预测的边界框重合，则认为该样本被正确召回。 检测效果图 # 最后的十四张图片就是检测效果图了，给大家看一下这里没什么好讲解的了。\n四、其它参数 # FPS和IoU是目标检测领域中使用的两个重要指标，分别表示每秒处理的图片数量和交并比。\nFPS：全称为Frames Per Second，即每秒帧率。它用于评估模型在给定硬件上的处理速度，即每秒可以处理的图片数量。该指标对于实现实时检测非常重要，因为只有处理速度快，才能满足实时检测的需求。 IoU：全称为Intersection over Union，表示交并比。在目标检测中，它用于衡量模型生成的候选框与原标记框之间的重叠程度。IoU值越大，表示两个框之间的相似性越高。通常，当IoU值大于0.5时，认为可以检测到目标物体。这个指标常用于评估模型在特定数据集上的检测准确度。 在目标检测领域中，处理速度和准确度是两个重要的性能指标。在实际应用中，我们需要根据具体需求来平衡这两个指标。\n","date":"2024-10-03","externalUrl":null,"permalink":"/docs/zhttr/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e一、简介 \n    \u003cdiv id=\"%E4%B8%80%E7%AE%80%E4%BB%8B\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%80%E7%AE%80%E4%BB%8B\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e这篇博客，\u003cstrong\u003e主要给大家讲解我们在训练yolov8时生成的结果文件中各个图片及其中指标的含义\u003c/strong\u003e，帮助大家更深入的理解，以及我们在评估模型时和发表论文时主要关注的参数有那些。本文通过举例训练过程中的某一时间的结果来帮助大家理解，大家阅读过程中如有任何问题可以在评论区提问出来，我会帮助大家解答。首先我们来看一个在一次训练完成之后都能生成多少个文件如下图所示，下面的文章讲解都会围绕这个结果文件来介绍。\u003c/p\u003e","title":"评估","type":"docs"},{"content":" 一、本文介绍 # 本文给大家带来的机制是集成了YOLOv8最新版本的可视化热力图功能，热力图作为我们论文当中的必备一环，可以展示出我们呈现机制的有效性，本文的内容支持YOLOv8最新版本的根据密度呈现的热力图，同时支持视频检测，根据视频中的密度来绘画热力图。\n在开始之前给大家推荐一下我的专栏，本专栏每周更新3-10篇最新前沿机制 | 包括二次创新全网无重复，以及融合改进(大家拿到之后添加另外一个改进机制在你的数据集上实现涨点即可撰写论文)，还有各种前沿顶会改进机制 |，更有包含我所有附赠的文件（文件内集成我所有的改进机制全部注册完毕可以直接运行）和交流群和视频讲解提供给大家。 欢迎大家订阅我的专栏一起学习YOLO！ 二、项目完整代码 # 我们将这个代码，复制粘贴到我们YOLOv8的仓库里然后创建一个py文件存放进去即可。\nfrom ultralytics import YOLO from ultralytics.solutions import heatmap import cv2 model = YOLO(\u0026#34;yolov8n.pt\u0026#34;) cap = cv2.VideoCapture(\u0026#34;People.mp4\u0026#34;) assert cap.isOpened(), \u0026#34;Error reading video file\u0026#34; w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS)) # Video writer video_writer = cv2.VideoWriter(\u0026#34;heatmap_output.avi\u0026#34;, cv2.VideoWriter_fourcc(*\u0026#39;mp4v\u0026#39;), fps, (w, h)) # Init heatmap heatmap_obj = heatmap.Heatmap() heatmap_obj.set_args(colormap=cv2.COLORMAP_PARULA, imw=w, imh=h, view_img=True, shape=\u0026#34;circle\u0026#34;) while cap.isOpened(): success, im0 = cap.read() if not success: print(\u0026#34;Video frame is empty or video processing has been successfully completed.\u0026#34;) break tracks = model.track(im0, persist=True, show=False) im0 = heatmap_obj.generate_heatmap(im0, tracks) video_writer.write(im0) cap.release() video_writer.release() cv2.destroyAllWindows() 三、参数解析 # 下面上面项目核心代码的参数解析，共有2个，能够起到作用的参数并不多。 参数名 参数类型 参数讲解 1 weights str 用于检测视频的权重文件地址（可以是你训练好的，也可以是官方提供的） 2 source str 视频文件的地址，因为是用于视频检测，大家有兴趣其实可以将其改为摄像头的实时检测。 四、项目的使用教程 # 4.1 步骤一 # 我们在Yolo仓库的目录下创建一个py文件将代码存放进去，如下图所示。\n4.2 步骤二 # 我们按照参数解析部分的介绍填好大家的参数，主要配置的有两个一个就是权重文件地址另一个就是视频的地址。\n4.3 步骤三 # 我们填好之后运行文件即可，运行文件即可，这里就不上传视频了。\n五、图片可视化热力图代码 # 下面的是图片热力图可视化代码，大家复制粘贴使用方法和上面差不多，代码中我有注释包含了使用方法大家可以仔细看一下，使用方法非常简单。\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) warnings.simplefilter(\u0026#39;ignore\u0026#39;) import torch, yaml, cv2, os, shutil import numpy as np np.random.seed(0) import matplotlib.pyplot as plt from tqdm import trange from PIL import Image from ultralytics.nn.tasks import DetectionModel as Model from ultralytics.utils.torch_utils import intersect_dicts from ultralytics.utils.ops import xywh2xyxy from pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM from pytorch_grad_cam.utils.image import show_cam_on_image from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): # Resize and pad image while meeting stride-multiple constraints shape = im.shape[:2] # current shape [height, width] if isinstance(new_shape, int): new_shape = (new_shape, new_shape) # Scale ratio (new / old) r = min(new_shape[0] / shape[0], new_shape[1] / shape[1]) if not scaleup: # only scale down, do not scale up (for better val mAP) r = min(r, 1.0) # Compute padding ratio = r, r # width, height ratios new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r)) dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1] # wh padding if auto: # minimum rectangle dw, dh = np.mod(dw, stride), np.mod(dh, stride) # wh padding elif scaleFill: # stretch dw, dh = 0.0, 0.0 new_unpad = (new_shape[1], new_shape[0]) ratio = new_shape[1] / shape[1], new_shape[0] / shape[0] # width, height ratios dw /= 2 # divide padding into 2 sides dh /= 2 if shape[::-1] != new_unpad: # resize im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR) top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1)) left, right = int(round(dw - 0.1)), int(round(dw + 0.1)) im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color) # add border return im, ratio, (dw, dh) class yolov8_heatmap: def __init__(self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio): device = torch.device(device) ckpt = torch.load(weight) model_names = ckpt[\u0026#39;model\u0026#39;].names csd = ckpt[\u0026#39;model\u0026#39;].float().state_dict() # checkpoint state_dict as FP32 model = Model(cfg, ch=3, nc=len(model_names)).to(device) csd = intersect_dicts(csd, model.state_dict(), exclude=[\u0026#39;anchor\u0026#39;]) # intersect model.load_state_dict(csd, strict=False) # load model.eval() print(f\u0026#39;Transferred {len(csd)}/{len(model.state_dict())} items\u0026#39;) target_layers = [eval(layer)] method = eval(method) colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32) self.__dict__.update(locals()) def post_process(self, result): logits_ = result[:, 4:] boxes_ = result[:, :4] sorted, indices = torch.sort(logits_.max(1)[0], descending=True) return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy() def draw_detections(self, box, color, name, img): xmin, ymin, xmax, ymax = list(map(int, list(box))) cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2) cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA) return img def __call__(self, img_path, save_path): # remove dir if exist if os.path.exists(save_path): shutil.rmtree(save_path) # make dir if not exist os.makedirs(save_path, exist_ok=True) # img process img = cv2.imread(img_path) img = letterbox(img)[0] img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = np.float32(img) / 255.0 tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device) # init ActivationsAndGradients grads = ActivationsAndGradients(self.model, self.target_layers, reshape_transform=None) # get ActivationsAndResult result = grads(tensor) activations = grads.activations[0].cpu().detach().numpy() # postprocess to yolo output post_result, pre_post_boxes, post_boxes = self.post_process(result[0]) for i in trange(int(post_result.size(0) * self.ratio)): if float(post_result[i].max()) \u0026lt; self.conf_threshold: break self.model.zero_grad() # get max probability for this prediction if self.backward_type == \u0026#39;class\u0026#39; or self.backward_type == \u0026#39;all\u0026#39;: score = post_result[i].max() score.backward(retain_graph=True) if self.backward_type == \u0026#39;box\u0026#39; or self.backward_type == \u0026#39;all\u0026#39;: for j in range(4): score = pre_post_boxes[i, j] score.backward(retain_graph=True) # process heatmap if self.backward_type == \u0026#39;class\u0026#39;: gradients = grads.gradients[0] elif self.backward_type == \u0026#39;box\u0026#39;: gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] else: gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] + grads.gradients[4] b, k, u, v = gradients.size() weights = self.method.get_cam_weights(self.method, None, None, None, activations, gradients.detach().numpy()) weights = weights.reshape((b, k, 1, 1)) saliency_map = np.sum(weights * activations, axis=1) saliency_map = np.squeeze(np.maximum(saliency_map, 0)) saliency_map = cv2.resize(saliency_map, (tensor.size(3), tensor.size(2))) saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max() if (saliency_map_max - saliency_map_min) == 0: continue saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min) # add heatmap and box to image cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=True) \u0026#34;不想在图片中绘画出边界框和置信度，注释下面的一行代码即可\u0026#34; cam_image = self.draw_detections(post_boxes[i], self.colors[int(post_result[i, :].argmax())], f\u0026#39;{self.model_names[int(post_result[i, :].argmax())]} {float(post_result[i].max()):.2f}\u0026#39;, cam_image) cam_image = Image.fromarray(cam_image) cam_image.save(f\u0026#39;{save_path}/{i}.png\u0026#39;) def get_params(): params = { \u0026#39;weight\u0026#39;: \u0026#39;yolov8n.pt\u0026#39;, # 训练出来的权重文件 \u0026#39;cfg\u0026#39;: \u0026#39;ultralytics/cfg/models/v8/yolov8n.yaml\u0026#39;, # 训练权重对应的yaml配置文件 \u0026#39;device\u0026#39;: \u0026#39;cuda:0\u0026#39;, \u0026#39;method\u0026#39;: \u0026#39;GradCAM\u0026#39;, # GradCAMPlusPlus, GradCAM, XGradCAM , 使用的热力图库文件不同的效果不一样可以多尝试 \u0026#39;layer\u0026#39;: \u0026#39;model.model[9]\u0026#39;, # 想要检测的对应层 \u0026#39;backward_type\u0026#39;: \u0026#39;all\u0026#39;, # class, box, all \u0026#39;conf_threshold\u0026#39;: 0.01, # 0.6 # 置信度阈值，有的时候你的进度条到一半就停止了就是因为没有高于此值的了 \u0026#39;ratio\u0026#39;: 0.02 # 0.02-0.1 } return params if __name__ == \u0026#39;__main__\u0026#39;: model = yolov8_heatmap(**get_params()) model(r\u0026#39;ultralytics/assets/bus.jpg\u0026#39;, \u0026#39;result\u0026#39;) # 第一个是检测的文件, 第二个是保存的路径 六、项目完整代码 # 我们将这个代码，复制粘贴到我们YOLOv8的仓库里然后创建一个py文件存放进去即可。\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) warnings.simplefilter(\u0026#39;ignore\u0026#39;) import torch, yaml, cv2, os, shutil import numpy as np np.random.seed(0) import matplotlib.pyplot as plt from tqdm import trange from PIL import Image from ultralytics.nn.tasks import DetectionModel as Model from ultralytics.utils.torch_utils import intersect_dicts from ultralytics.utils.ops import xywh2xyxy from pytorch_grad_cam import GradCAMPlusPlus, GradCAM, XGradCAM from pytorch_grad_cam.utils.image import show_cam_on_image from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32): # Resize and pad image while meeting stride-multiple constraints shape = im.shape[:2] # current shape [height, width] if isinstance(new_shape, int): new_shape = (new_shape, new_shape) # Scale ratio (new / old) r = min(new_shape[0] / shape[0], new_shape[1] / shape[1]) if not scaleup: # only scale down, do not scale up (for better val mAP) r = min(r, 1.0) # Compute padding ratio = r, r # width, height ratios new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r)) dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1] # wh padding if auto: # minimum rectangle dw, dh = np.mod(dw, stride), np.mod(dh, stride) # wh padding elif scaleFill: # stretch dw, dh = 0.0, 0.0 new_unpad = (new_shape[1], new_shape[0]) ratio = new_shape[1] / shape[1], new_shape[0] / shape[0] # width, height ratios dw /= 2 # divide padding into 2 sides dh /= 2 if shape[::-1] != new_unpad: # resize im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR) top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1)) left, right = int(round(dw - 0.1)), int(round(dw + 0.1)) im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color) # add border return im, ratio, (dw, dh) class yolov8_heatmap: def __init__(self, weight, cfg, device, method, layer, backward_type, conf_threshold, ratio): device = torch.device(device) ckpt = torch.load(weight) model_names = ckpt[\u0026#39;model\u0026#39;].names csd = ckpt[\u0026#39;model\u0026#39;].float().state_dict() # checkpoint state_dict as FP32 model = Model(cfg, ch=3, nc=len(model_names)).to(device) csd = intersect_dicts(csd, model.state_dict(), exclude=[\u0026#39;anchor\u0026#39;]) # intersect model.load_state_dict(csd, strict=False) # load model.eval() print(f\u0026#39;Transferred {len(csd)}/{len(model.state_dict())} items\u0026#39;) target_layers = [eval(layer)] method = eval(method) colors = np.random.uniform(0, 255, size=(len(model_names), 3)).astype(np.int32) self.__dict__.update(locals()) def post_process(self, result): logits_ = result[:, 4:] boxes_ = result[:, :4] sorted, indices = torch.sort(logits_.max(1)[0], descending=True) return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy() def draw_detections(self, box, color, name, img): xmin, ymin, xmax, ymax = list(map(int, list(box))) cv2.rectangle(img, (xmin, ymin), (xmax, ymax), tuple(int(x) for x in color), 2) cv2.putText(img, str(name), (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, tuple(int(x) for x in color), 2, lineType=cv2.LINE_AA) return img def __call__(self, img_path, save_path): # remove dir if exist if os.path.exists(save_path): shutil.rmtree(save_path) # make dir if not exist os.makedirs(save_path, exist_ok=True) # img process img = cv2.imread(img_path) img = letterbox(img)[0] img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = np.float32(img) / 255.0 tensor = torch.from_numpy(np.transpose(img, axes=[2, 0, 1])).unsqueeze(0).to(self.device) # init ActivationsAndGradients grads = ActivationsAndGradients(self.model, self.target_layers, reshape_transform=None) # get ActivationsAndResult result = grads(tensor) activations = grads.activations[0].cpu().detach().numpy() # postprocess to yolo output post_result, pre_post_boxes, post_boxes = self.post_process(result[0]) for i in trange(int(post_result.size(0) * self.ratio)): if float(post_result[i].max()) \u0026lt; self.conf_threshold: break self.model.zero_grad() # get max probability for this prediction if self.backward_type == \u0026#39;class\u0026#39; or self.backward_type == \u0026#39;all\u0026#39;: score = post_result[i].max() score.backward(retain_graph=True) if self.backward_type == \u0026#39;box\u0026#39; or self.backward_type == \u0026#39;all\u0026#39;: for j in range(4): score = pre_post_boxes[i, j] score.backward(retain_graph=True) # process heatmap if self.backward_type == \u0026#39;class\u0026#39;: gradients = grads.gradients[0] elif self.backward_type == \u0026#39;box\u0026#39;: gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] else: gradients = grads.gradients[0] + grads.gradients[1] + grads.gradients[2] + grads.gradients[3] + grads.gradients[4] b, k, u, v = gradients.size() weights = self.method.get_cam_weights(self.method, None, None, None, activations, gradients.detach().numpy()) weights = weights.reshape((b, k, 1, 1)) saliency_map = np.sum(weights * activations, axis=1) saliency_map = np.squeeze(np.maximum(saliency_map, 0)) saliency_map = cv2.resize(saliency_map, (tensor.size(3), tensor.size(2))) saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max() if (saliency_map_max - saliency_map_min) == 0: continue saliency_map = (saliency_map - saliency_map_min) / (saliency_map_max - saliency_map_min) # add heatmap and box to image cam_image = show_cam_on_image(img.copy(), saliency_map, use_rgb=True) \u0026#34;不想在图片中绘画出边界框和置信度，注释下面的一行代码即可\u0026#34; cam_image = self.draw_detections(post_boxes[i], self.colors[int(post_result[i, :].argmax())], f\u0026#39;{self.model_names[int(post_result[i, :].argmax())]} {float(post_result[i].max()):.2f}\u0026#39;, cam_image) cam_image = Image.fromarray(cam_image) cam_image.save(f\u0026#39;{save_path}/{i}.png\u0026#39;) def get_params(): params = { \u0026#39;weight\u0026#39;: \u0026#39;yolov8n.pt\u0026#39;, # 训练出来的权重文件 \u0026#39;cfg\u0026#39;: \u0026#39;ultralytics/cfg/models/v8/yolov8n.yaml\u0026#39;, # 训练权重对应的yaml配置文件 \u0026#39;device\u0026#39;: \u0026#39;cuda:0\u0026#39;, \u0026#39;method\u0026#39;: \u0026#39;GradCAM\u0026#39;, # GradCAMPlusPlus, GradCAM, XGradCAM , 使用的热力图库文件不同的效果不一样可以多尝试 \u0026#39;layer\u0026#39;: \u0026#39;model.model[9]\u0026#39;, # 想要检测的对应层 \u0026#39;backward_type\u0026#39;: \u0026#39;all\u0026#39;, # class, box, all \u0026#39;conf_threshold\u0026#39;: 0.01, # 0.6 # 置信度阈值，有的时候你的进度条到一半就停止了就是因为没有高于此值的了 \u0026#39;ratio\u0026#39;: 0.02 # 0.02-0.1 } return params if __name__ == \u0026#39;__main__\u0026#39;: model = yolov8_heatmap(**get_params()) model(r\u0026#39;ultralytics/assets/bus.jpg\u0026#39;, \u0026#39;result\u0026#39;) # 第一个是检测的文件, 第二个是保存的路径 七、参数解析 # 下面上面项目核心代码的参数解析，共有7个，能够起到作用的参数并不多。 参数名 参数类型 参数讲解 0 weights str 用于检测视频的权重文件地址（可以是你训练好的，也可以是官方提供的） 1 cfg str 你选择的权重对应的yaml配置文件，请注意一定要对应否则会报错和不显示图片 2 device str 设备的选择可以用GPU也可以用CPU 3 method str 使用的热力图第三方库的版本，不同的版本效果也不一样。 4 layer str 想要检测的对应层，比如这里设置的是9那么检测的就是第九层 4 backward_type str 检测的类别 5 conf_threshold str 置信度阈值，有的时候你的进度条没有满就是因为没有大于这个阈值的图片了 6 ratio int YOLOv8一次产生6300张预测框，选择多少比例的图片绘画热力图。 八、项目的使用教程 # 8.1 步骤一 # 我们在Yolo仓库的目录下创建一个py文件将代码存放进去，如下图所示。\n8.2 步骤二 # 我们按照参数解析部分的介绍填好大家的参数，主要配置的有两个一个就是权重文件地址另一个就是图片的地址。\n8.3 步骤三 # 我们挺好之后运行文件即可，图片就会保存在同级目录下的新的文件夹result内。\n8.4 置信度和检测框 # 看下下面的说明就行。\n","date":"2024-10-03","externalUrl":null,"permalink":"/docs/tree/","section":"文档","summary":"\u003ch2 class=\"relative group\"\u003e一、本文介绍 \n    \u003cdiv id=\"%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%E4%B8%80%E6%9C%AC%E6%96%87%E4%BB%8B%E7%BB%8D\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e本文给大家带来的机制是集成了YOLOv8最新版本的可视化热力图功能，热力图作为我们论文当中的必备一环，可以展示出我们呈现机制的有效性，本文的内容支持YOLOv8最新版本的根据密度呈现的热力图，同时支持视频检测，根据视频中的密度来绘画热力图。\u003c/p\u003e","title":"热力图分析","type":"docs"},{"content":"","externalUrl":"https://madoke.org/","permalink":"/users/madoke.org/","section":"用户列表","summary":"","title":"madoke.org","type":"users"},{"content":"","externalUrl":"https://code-chimp.com","permalink":"/users/code-chimp.com/","section":"用户列表","summary":"","title":"code-chimp.com","type":"users"},{"content":"","externalUrl":"https://mucahitkurtlar.github.io","permalink":"/users/mucahitkurtlar.github.io/","section":"用户列表","summary":"","title":"mucahitkurtlar.github.io","type":"users"},{"content":"","externalUrl":"https://brendanwallace.github.io","permalink":"/users/brendanwallace.github.io/","section":"用户列表","summary":"","title":"brendanwallace.github.io","type":"users"},{"content":"","externalUrl":"https://cuttontail.blog","permalink":"/users/cuttontail.blog/","section":"用户列表","summary":"","title":"cuttontail.blog","type":"users"},{"content":"","externalUrl":"https://pmnxis.github.io","permalink":"/users/pmnxis.github.io/","section":"用户列表","summary":"","title":"pmnxis.github.io","type":"users"},{"content":"","externalUrl":"https://ciicadalab.github.io","permalink":"/users/ciicadalab.github.io/","section":"用户列表","summary":"","title":"ciicadalab.github.io","type":"users"},{"content":"","externalUrl":"https://georgiancodeclub.github.io","permalink":"/users/georgiancodeclub.github.io/","section":"用户列表","summary":"","title":"georgiancodeclub.github.io","type":"users"},{"content":"","externalUrl":"https://albertolvera.com","permalink":"/users/albertolvera.com/","section":"用户列表","summary":"","title":"albertolvera.com","type":"users"},{"content":"","externalUrl":"https://www.fahru.my.id","permalink":"/users/fahru.my.id/","section":"用户列表","summary":"","title":"fahru.my.id","type":"users"},{"content":"","externalUrl":"https://loisvelasco.is-a.dev","permalink":"/users/loisvelasco.is-a.dev/","section":"用户列表","summary":"","title":"loisvelasco.is-a.dev","type":"users"},{"content":"","externalUrl":"https://omarohn.de","permalink":"/users/omarohn.de/","section":"用户列表","summary":"","title":"omarohn.de","type":"users"},{"content":"","externalUrl":"https://insidemordecai.com","permalink":"/users/insidemordecai.com/","section":"用户列表","summary":"","title":"insidemordecai.com","type":"users"},{"content":"","externalUrl":"https://blastomussa.dev","permalink":"/users/blastomussa.dev/","section":"用户列表","summary":"","title":"blastomussa.dev","type":"users"},{"content":"","externalUrl":"https://cdell.io","permalink":"/users/cdell.io/","section":"用户列表","summary":"","title":"cdell.io","type":"users"},{"content":"","externalUrl":"https://jam.dsg.li","permalink":"/users/jam.dsg.li/","section":"用户列表","summary":"","title":"jam.dsg.li","type":"users"},{"content":"","externalUrl":"https://priyakdey.com","permalink":"/users/priyakdey.com/","section":"用户列表","summary":"","title":"priyakdey.com","type":"users"},{"content":"","externalUrl":"https://sdehm.dev","permalink":"/users/sdehm.dev/","section":"用户列表","summary":"","title":"sdehm.dev","type":"users"},{"content":"","externalUrl":"https://dizzytech.de","permalink":"/users/dizzytech.de/","section":"用户列表","summary":"","title":"dizzytech.de","type":"users"},{"content":"","externalUrl":"https://alejandro-ao.com/","permalink":"/users/alejandro-ao.com/","section":"用户列表","summary":"","title":"alejandro-ao.com","type":"users"},{"content":"","externalUrl":"https://adir1.com/","permalink":"/users/adir1.com/","section":"用户列表","summary":"","title":"adir1.com","type":"users"},{"content":"","externalUrl":"https://niklas-hartmann-dev.de/","permalink":"/users/niklas-hartmann-dev.de/","section":"用户列表","summary":"","title":"niklas-hartmann-dev.de","type":"users"},{"content":"","externalUrl":"https://blog.muffn.io/","permalink":"/users/blog.muffn.io/","section":"用户列表","summary":"","title":"blog.muffn.io","type":"users"},{"content":"","externalUrl":"https://nick.bouwhuis.net","permalink":"/users/nick.bouwhuis.net/","section":"用户列表","summary":"","title":"nick.bouwhuis.net","type":"users"},{"content":"","externalUrl":"https://vividscc.com/","permalink":"/users/vividscc.com/","section":"用户列表","summary":"","title":"vividscc.com","type":"users"},{"content":"","externalUrl":"https://mariuskimmina.com/","permalink":"/users/mariuskimmina.com/","section":"用户列表","summary":"","title":"mariuskimmina.com","type":"users"},{"content":"","externalUrl":"https://technicat.com/","permalink":"/users/technicat.com/","section":"用户列表","summary":"","title":"technicat.com","type":"users"},{"content":"","externalUrl":"https://fugugames.com/","permalink":"/users/fugugames.com/","section":"用户列表","summary":"","title":"fugugames.com","type":"users"},{"content":"","externalUrl":"https://hyperbowl3d.com/","permalink":"/users/hyperbowl3d.com/","section":"用户列表","summary":"","title":"hyperbowl3d.com","type":"users"},{"content":"","externalUrl":"https://talkdimsum.com/","permalink":"/users/talkdimsum.com/","section":"用户列表","summary":"","title":"talkdimsum.com","type":"users"},{"content":"","externalUrl":"https://alanctanner.com/","permalink":"/users/alanctanner.com/","section":"用户列表","summary":"","title":"alanctanner.com","type":"users"},{"content":"","externalUrl":"https://rdgo.dev/","permalink":"/users/rdgo.dev/","section":"用户列表","summary":"","title":"rdgo.dev","type":"users"},{"content":"","externalUrl":"https://clemsau.com/","permalink":"/users/clemsau.com/","section":"用户列表","summary":"","title":"clemsau.com","type":"users"},{"content":"","externalUrl":"https://lelouvincx.github.io/","permalink":"/users/lelouvincx.github.io/","section":"用户列表","summary":"","title":"lelouvincx.github.io","type":"users"},{"content":"","externalUrl":"https://weaxsey.org/","permalink":"/users/weaxsey.org/","section":"用户列表","summary":"","title":"weaxsey.org","type":"users"},{"content":"","externalUrl":"https://nikarashihatsu.github.io/","permalink":"/users/nikarashihatsu.github.io/","section":"用户列表","summary":"","title":"nikarashihatsu.github.io","type":"users"},{"content":"","externalUrl":"https://blog.enmanuelmoreira.com","permalink":"/users/blog.enmanuelmoreira.com/","section":"用户列表","summary":"","title":"blog.enmanuelmoreira.com","type":"users"},{"content":"","externalUrl":"https://www.halcyonstraits.com/","permalink":"/users/halcyonstraits.com/","section":"用户列表","summary":"","title":"halcyonstraits.com","type":"users"},{"content":"","externalUrl":"https://www.50-nuances-octets.fr/","permalink":"/users/50-nuances-octets.fr/","section":"用户列表","summary":"","title":"50-nuances-octets.fr","type":"users"},{"content":"","externalUrl":"https://marupanda.art/marucomics/","permalink":"/users/marupanda.art-marucomics/","section":"用户列表","summary":"","title":"marupanda.art/marucomics","type":"users"},{"content":"","externalUrl":"https://m3upt.com","permalink":"/users/m3upt.com/","section":"用户列表","summary":"","title":"m3upt.com","type":"users"},{"content":"","externalUrl":"https://pacochan.net","permalink":"/users/pacochan.net/","section":"用户列表","summary":"","title":"pacochan.net","type":"users"},{"content":"","externalUrl":"https://jundimubarok.com/","permalink":"/users/jundimubarok.com/","section":"用户列表","summary":"","title":"jundimubarok.com","type":"users"},{"content":"","externalUrl":"https://vkmki001.github.io/","permalink":"/users/vkmki001.github.io/","section":"用户列表","summary":"","title":"vkmki001.github.io","type":"users"},{"content":"","externalUrl":"https://bbagwang.com","permalink":"/users/bbagwang.com/","section":"用户列表","summary":"","title":"bbagwang.com","type":"users"},{"content":"","externalUrl":"https://jamiemoxon.tech","permalink":"/users/jamiemoxon.tech/","section":"用户列表","summary":"","title":"jamiemoxon.tech","type":"users"},{"content":"","externalUrl":"https://theindiecoder.cloud","permalink":"/users/theindiecoder.cloud/","section":"用户列表","summary":"","title":"theindiecoder.cloud","type":"users"},{"content":"","externalUrl":"https://gma.name","permalink":"/users/gma.name/","section":"用户列表","summary":"","title":"gma.name","type":"users"},{"content":"","externalUrl":"https://mayer.life","permalink":"/users/mayer.life/","section":"用户列表","summary":"","title":"mayer.life","type":"users"},{"content":"","externalUrl":"https://scottmckendry.tech","permalink":"/users/scottmckendry.tech/","section":"用户列表","summary":"","title":"scottmckendry.tech","type":"users"},{"content":"","externalUrl":"https://adilhyz.github.io","permalink":"/users/adilhyz.github.io/","section":"用户列表","summary":"","title":"adilhyz.github.io","type":"users"},{"content":"","externalUrl":"https://ohdmire.github.io","permalink":"/users/ohdmire.github.io/","section":"用户列表","summary":"","title":"ohdmire.github.io","type":"users"},{"content":"","externalUrl":"https://ricklan.photography","permalink":"/users/ricklan.photography/","section":"用户列表","summary":"","title":"ricklan.photography","type":"users"},{"content":"","externalUrl":"https://deepumohan.com/tech/","permalink":"/users/deepumohan.com-tech/","section":"用户列表","summary":"","title":"deepumohan.com/tech","type":"users"},{"content":"","externalUrl":"https://kylemalloy.com","permalink":"/users/kylemalloy.com/","section":"用户列表","summary":"","title":"kylemalloy.com","type":"users"},{"content":"","externalUrl":"https://joush007.github.io","permalink":"/users/joush007.github.io/","section":"用户列表","summary":"","title":"joush007.github.io","type":"users"},{"content":"","externalUrl":"https://rejowski.xyz/","permalink":"/users/rejowski.xyz/","section":"用户列表","summary":"","title":"rejowski.xyz","type":"users"},{"content":"","externalUrl":"https://v-y-s.com/","permalink":"/users/v-y-s.com/","section":"用户列表","summary":"","title":"v-y-s.com","type":"users"},{"content":"","externalUrl":"https://blog.stonegarden.dev/","permalink":"/users/blog.stonegarden.dev/","section":"用户列表","summary":"","title":"blog.stonegarden.dev","type":"users"},{"content":"","externalUrl":"https://renaud.warnotte.be","permalink":"/users/renaud.warnotte.be/","section":"用户列表","summary":"","title":"renaud.warnotte.be","type":"users"},{"content":"","externalUrl":"https://boringtech.net/","permalink":"/users/boringtech.net/","section":"用户列表","summary":"","title":"BoringTech.net","type":"users"},{"content":"","externalUrl":"https://technicaldc.github.io/","permalink":"/users/technicaldc.github.io/","section":"用户列表","summary":"","title":"technicaldc.github.io","type":"users"},{"content":"","externalUrl":"https://alxhslm.github.io/","permalink":"/users/alxhslm.github.io/","section":"用户列表","summary":"","title":"alxhslm.github.io","type":"users"},{"content":"","externalUrl":"https://www.the-maze.net/","permalink":"/users/the-maze.net/","section":"用户列表","summary":"","title":"the-maze.net","type":"users"},{"content":"","externalUrl":"https://www.dxpetti.com/","permalink":"/users/dxpetti.com/","section":"用户列表","summary":"","title":"DXPetti.com","type":"users"},{"content":"","externalUrl":"https://asterisk.lol","permalink":"/users/asterisk.lol/","section":"用户列表","summary":"","title":"asterisk.lol","type":"users"},{"content":"","externalUrl":"https://notes.bluesdriveamelia.space/","permalink":"/users/notes.bluesdriveamelia.space/","section":"用户列表","summary":"","title":"notes.bluesdriveamelia.space","type":"users"},{"content":"","externalUrl":"https://ekwska.com","permalink":"/users/ekwska.com/","section":"用户列表","summary":"","title":"ekwska.com","type":"users"},{"content":"","externalUrl":"https://todreamr.github.io/","permalink":"/users/todreamr.github.io/","section":"用户列表","summary":"","title":"todreamr.github.io","type":"users"},{"content":"","externalUrl":"https://aakashnand.com/","permalink":"/users/aakashnand.com/","section":"用户列表","summary":"","title":"aakashnand.com","type":"users"},{"content":"","externalUrl":"https://innerknowing.xyz/en/","permalink":"/users/innerknowing/","section":"用户列表","summary":"","title":"innerknowing","type":"users"},{"content":"","externalUrl":"https://lab.imgb.space","permalink":"/users/mare_infinitus/","section":"用户列表","summary":"","title":"Mare_Infinitus","type":"users"},{"content":"","externalUrl":"https://karlukle.site","permalink":"/users/karlukle.site/","section":"用户列表","summary":"","title":"karlukle.site","type":"users"},{"content":"","externalUrl":"http://www.adammadej.com/","permalink":"/users/adam-madej---gameplay-animator/","section":"用户列表","summary":"","title":"Adam Madej - Gameplay Animator","type":"users"},{"content":"","externalUrl":"http://www.eallion.com/","permalink":"/users/eallion.com/","section":"用户列表","summary":"","title":"eallion.com","type":"users"},{"content":"","externalUrl":"https://synapticsugar.games","permalink":"/users/synaptic-sugar/","section":"用户列表","summary":"","title":"Synaptic Sugar","type":"users"},{"content":"","externalUrl":"https://www.michaeldorner.de","permalink":"/users/michaeldorner.de/","section":"用户列表","summary":"","title":"michaeldorner.de","type":"users"},{"content":"","externalUrl":"http://www.ignaciomconde.com/","permalink":"/users/ignacio-conde/","section":"用户列表","summary":"","title":"Ignacio Conde","type":"users"},{"content":"","externalUrl":"https://memv.ennbee.uk/","permalink":"/users/mem-v-ennbee/","section":"用户列表","summary":"","title":"MEM v ENNBEE","type":"users"},{"content":"","externalUrl":"https://joshblais.com/","permalink":"/users/joshua-blais/","section":"用户列表","summary":"","title":"Joshua Blais","type":"users"},{"content":"","externalUrl":"https://www.beautyformulation.com/","permalink":"/users/beauty-formulation/","section":"用户列表","summary":"","title":"Beauty Formulation","type":"users"},{"content":"","externalUrl":"https://blog.wtcx.dev/","permalink":"/users/middle-of-nowhere/","section":"用户列表","summary":"","title":"Middle of Nowhere","type":"users"},{"content":"","externalUrl":"https://blog.ummit.dev/","permalink":"/users/ummit---blog/","section":"用户列表","summary":"","title":"UmmIt - Blog","type":"users"},{"content":"","externalUrl":"https://zen96k.github.io/enoshima-escar","permalink":"/users/%E6%B1%9F%E3%83%8E%E5%B3%B6%E3%82%A8%E3%82%B9%E3%82%AB%E3%83%BC/","section":"用户列表","summary":"","title":"江ノ島エスカー","type":"users"},{"content":"","externalUrl":"https://www.panjinbo.com/","permalink":"/users/jinbo-pan---blog/","section":"用户列表","summary":"","title":"Jinbo Pan - Blog","type":"users"},{"content":"","externalUrl":"https://nveshaan.github.io/","permalink":"/users/nveshaan/","section":"用户列表","summary":"","title":"nveshaan","type":"users"},{"content":"","externalUrl":"https://micheledinelli.github.io","permalink":"/users/micheledinelli.github.io/","section":"用户列表","summary":"","title":"micheledinelli.github.io","type":"users"},{"content":"","externalUrl":"https://kumacat.pages.dev","permalink":"/users/kumacat.pages.dev/","section":"用户列表","summary":"","title":"kumacat.pages.dev","type":"users"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/cnn/","section":"标签","summary":"","title":"CNN","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/fps/","section":"标签","summary":"","title":"FPS","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/gpu/","section":"标签","summary":"","title":"GPU","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/yaml/","section":"标签","summary":"","title":"Yaml","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/yolo/","section":"标签","summary":"","title":"YOLO","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/yolov8/","section":"标签","summary":"","title":"YOLOv8","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/%E5%AE%89%E8%A3%85/","section":"标签","summary":"","title":"安装","type":"tags"},{"content":"Blowfish 支持基于 Hugo 的所有分类方法。同时，当前的标签预览页也支持展示自定义内容。\n在这里可以为每个分类添加额外的描述信息。查看下面的高级标签页面，了解更多。\n","date":"2024-10-03","externalUrl":null,"permalink":"/tags/","section":"标签","summary":"\u003cp\u003eBlowfish 支持基于 Hugo 的所有分类方法。同时，当前的标签预览页也支持展示自定义内容。\u003c/p\u003e","title":"标签","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/%E8%AF%84%E4%BC%B0/","section":"标签","summary":"","title":"评估","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/","section":"标签","summary":"","title":"评估指标","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/%E7%83%AD%E5%8A%9B%E5%9B%BE/","section":"标签","summary":"","title":"热力图","type":"tags"},{"content":"","date":"2024-10-03","externalUrl":null,"permalink":"/tags/%E6%96%87%E6%A1%A3/","section":"标签","summary":"","title":"文档","type":"tags"},{"content":" 了解如何使用简单而强大的YOLOv8。 ","date":"2024-10-03","externalUrl":null,"permalink":"/docs/","section":"文档","summary":"\u003cdiv class=\"lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl\"\u003e\n  了解如何使用简单而强大的YOLOv8。\n\u003c/div\u003e\n\n\u003chr\u003e","title":"文档","type":"docs"},{"content":"","date":"2024-10-01","externalUrl":"https://github.com/Louaq","permalink":"/pytorch/","section":"Pytorch使用教程","summary":"","title":"Pytorch使用教程","type":"pytorch"},{"content":"p\n","date":"2024-10-01","externalUrl":null,"permalink":"/pytorch/section_1/","section":"Pytorch使用教程","summary":"\u003cp\u003ep\u003c/p\u003e","title":"第一节","type":"pytorch"},{"content":"","date":"2024-10-01","externalUrl":"https://github.com/Louaq","permalink":"/","section":"主页","summary":"","title":"主页","type":"page"},{"content":"","date":"2023-10-02","externalUrl":"https://blowfish-tutorial.web.app/","permalink":"/examples/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Tutorial","type":"examples"},{"content":"","date":"2023-10-02","externalUrl":"https://blowfish-tutorial.web.app/","permalink":"/examples/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Tutorial","type":"examples"},{"content":"","date":"2023-10-02","externalUrl":"https://blowfish-tutorial.web.app/","permalink":"/examples/blowfish-tutorial/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Tutorial","type":"examples"},{"content":"","date":"2023-10-02","externalUrl":"https://blowfish-tutorial.web.app/","permalink":"/examples/blowfish-tutorial/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Tutorial","type":"examples"},{"content":"","date":"2023-10-02","externalUrl":"https://blowfish-tutorial.web.app/","permalink":"/examples/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish チュートリアル","type":"examples"},{"content":"","date":"2023-10-02","externalUrl":"https://blowfish-tutorial.web.app/","permalink":"/examples/blowfish-tutorial/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish チュートリアル","type":"examples"},{"content":"","date":"2023-10-01","externalUrl":"https://github.com/nunocoracao/blowfish-tutorial","permalink":"/examples/repo-blowfish-tutorial/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Tutorial - Repo","type":"examples"},{"content":"","date":"2023-10-01","externalUrl":"https://github.com/nunocoracao/blowfish-tutorial","permalink":"/examples/repo-blowfish-tutorial/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Tutorial - Repo","type":"examples"},{"content":"","date":"2023-10-01","externalUrl":"https://github.com/nunocoracao/blowfish-tutorial","permalink":"/examples/repo-blowfish-tutorial/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish チュートリアル - レポジトリ","type":"examples"},{"content":"","date":"2022-11-07","externalUrl":"https://nunocoracao.github.io/blowfish_lite/","permalink":"/examples/blowfish-lite/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lite","type":"examples"},{"content":"","date":"2022-11-07","externalUrl":"https://nunocoracao.github.io/blowfish_lite/","permalink":"/examples/blowfish-lite/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lite","type":"examples"},{"content":"","date":"2022-11-07","externalUrl":"https://nunocoracao.github.io/blowfish_lite/","permalink":"/examples/blowfish-lite/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lite","type":"examples"},{"content":"","date":"2022-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_artist/","permalink":"/examples/blowfish-artist/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Artist","type":"examples"},{"content":"","date":"2022-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_artist/","permalink":"/examples/blowfish-artist/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Artist","type":"examples"},{"content":"","date":"2022-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_artist/","permalink":"/examples/blowfish-artist/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Artist","type":"examples"},{"content":"","date":"2022-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_lowkey/","permalink":"/examples/blowfish-lowkey/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lowkey","type":"examples"},{"content":"","date":"2022-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_lowkey/","permalink":"/examples/blowfish-lowkey/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lowkey","type":"examples"},{"content":"","date":"2022-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_lowkey/","permalink":"/examples/blowfish-lowkey/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lowkey","type":"examples"},{"content":"","date":"2021-11-07","externalUrl":"https://github.com/nunocoracao/blowfish_lite/","permalink":"/examples/repo-blowfish-lite/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lite - Repo","type":"examples"},{"content":"","date":"2021-11-07","externalUrl":"https://github.com/nunocoracao/blowfish_lite/","permalink":"/examples/repo-blowfish-lite/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lite - Repo","type":"examples"},{"content":"","date":"2021-11-07","externalUrl":"https://github.com/nunocoracao/blowfish_lite/","permalink":"/examples/repo-blowfish-lite/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lite - レポジトリ","type":"examples"},{"content":"","date":"2021-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_artist/","permalink":"/examples/repo-blowfish-artist/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Artist - Repo","type":"examples"},{"content":"","date":"2021-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_artist/","permalink":"/examples/repo-blowfish-artist/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Artist - Repo","type":"examples"},{"content":"","date":"2021-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_artist/","permalink":"/examples/repo-blowfish-artist/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Artist - レポジトリ","type":"examples"},{"content":"","date":"2021-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_lowkey/","permalink":"/examples/repo-blowfish-lowkey/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lowkey - Repo","type":"examples"},{"content":"","date":"2021-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_lowkey/","permalink":"/examples/repo-blowfish-lowkey/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lowkey - Repo","type":"examples"},{"content":"","date":"2021-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_lowkey/","permalink":"/examples/repo-blowfish-lowkey/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Lowkey - レポジトリ","type":"examples"},{"content":"","date":"2020-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_template/","permalink":"/examples/blowfish-template/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Template","type":"examples"},{"content":"","date":"2020-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_template/","permalink":"/examples/blowfish-template/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Template","type":"examples"},{"content":"","date":"2020-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_template","permalink":"/examples/blowfish-template-repo/_index.it/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Template - GitHub Repo","type":"examples"},{"content":"","date":"2020-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_template","permalink":"/examples/blowfish-template-repo/","section":"Blowfish Tutorial","summary":"","title":"Blowfish Template - GitHub Repo","type":"examples"},{"content":"","date":"2020-11-06","externalUrl":"https://nunocoracao.github.io/blowfish_template/","permalink":"/examples/blowfish-template/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish テンプレート","type":"examples"},{"content":"","date":"2020-11-06","externalUrl":"https://github.com/nunocoracao/blowfish_template","permalink":"/examples/blowfish-template-repo/_index.ja/","section":"Blowfish Tutorial","summary":"","title":"Blowfish テンプレート - GitHub レポジトリ","type":"examples"},{"content":"","date":"2020-08-14","externalUrl":null,"permalink":"/tags/sample/","section":"标签","summary":"","title":"Sample","type":"tags"},{"content":"","date":"2020-08-14","externalUrl":null,"permalink":"/tags/users/","section":"标签","summary":"","title":"Users","type":"tags"},{"content":"Real websites that are built with Blowfish. Check the full list in JSON format.\nBlowfish user? To add your site to this list, submit a pull request. ","date":"2020-08-14","externalUrl":null,"permalink":"/users/users/","section":"用户列表","summary":"\u003cp\u003eReal websites that are built with Blowfish. Check the full list in \u003ca href=\"/users/users.json\"\u003eJSON format\u003c/a\u003e.\u003c/p\u003e\n\n  \n\n\n\n\u003cdiv\n  \n    class=\"flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900\"\n  \u003e\n\n  \u003cspan\n    \n      class=\"text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center\"\n    \u003e\n\n    \n\n  \u003cspan class=\"relative block icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M506.3 417l-213.3-364c-16.33-28-57.54-28-73.98 0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6C502.1 480 522.6 445 506.3 417zM232 168c0-13.25 10.75-24 24-24S280 154.8 280 168v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zM256 416c-17.36 0-31.44-14.08-31.44-31.44c0-17.36 14.07-31.44 31.44-31.44s31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n\n  \u003c/span\u003e\n\n  \u003cspan\n    \n      class=\"dark:text-neutral-300\"\n    \u003e\u003cstrong\u003eBlowfish user?\u003c/strong\u003e To add your site to this list, \u003ca href=\"https://github.com/nunocoracao/blowfish/blob/dev/exampleSite/content/users/users.json\" target=\"_blank\"\u003esubmit a pull request\u003c/a\u003e.\u003c/span\u003e\n\u003c/div\u003e\n\n\u003c/BR\u003e","title":"Users","type":"users"},{"content":"Blowfish で構築された実際のウェブサイトです。すべてのウェブサイト一覧は JSON 形式 で閲覧可能です。\nBlowfish ユーザーですか? この一覧にあなたのサイトを加える際は、 pull request を送信 してください。 ","date":"2020-08-14","externalUrl":null,"permalink":"/users/users/","section":"用户列表","summary":"\u003cp\u003eBlowfish で構築された実際のウェブサイトです。すべてのウェブサイト一覧は \u003ca href=\"/users/users.json\"\u003eJSON 形式\u003c/a\u003e で閲覧可能です。\u003c/p\u003e","title":"ユーザー","type":"users"},{"content":"","date":"2020-08-14","externalUrl":null,"permalink":"/tags/%E7%A4%BA%E4%BE%8B/","section":"标签","summary":"","title":"示例","type":"tags"},{"content":"这里有使用 Blowfish 构建的网站实例。点击这里以 JSON 形式查看完整列表。\n您也是 Blowfish 用户？ 提交 PR来把你的网站加入此列表。 ","date":"2020-08-14","externalUrl":null,"permalink":"/users/","section":"用户列表","summary":"\u003cp\u003e这里有使用 Blowfish 构建的网站实例。点击\u003ca href=\"/users/users.json\"\u003e这里\u003c/a\u003e以 JSON 形式查看完整列表。\u003c/p\u003e\n\n  \n\n\n\n\u003cdiv\n  \n    class=\"flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900\"\n  \u003e\n\n  \u003cspan\n    \n      class=\"text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center\"\n    \u003e\n\n    \n\n  \u003cspan class=\"relative block icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M506.3 417l-213.3-364c-16.33-28-57.54-28-73.98 0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6C502.1 480 522.6 445 506.3 417zM232 168c0-13.25 10.75-24 24-24S280 154.8 280 168v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zM256 416c-17.36 0-31.44-14.08-31.44-31.44c0-17.36 14.07-31.44 31.44-31.44s31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n\n  \u003c/span\u003e\n\n  \u003cspan\n    \n      class=\"dark:text-neutral-300\"\n    \u003e\u003cstrong\u003e您也是 Blowfish 用户？\u003c/strong\u003e \u003ca href=\"https://github.com/nunocoracao/blowfish/blob/dev/exampleSite/content/users/users.json\" target=\"_blank\"\u003e提交 PR\u003c/a\u003e来把你的网站加入此列表。\u003c/span\u003e\n\u003c/div\u003e\n\n\u003c/BR\u003e","title":"用户列表","type":"users"},{"content":"","date":"2020-08-14","externalUrl":null,"permalink":"/tags/%E5%8F%8B%E9%93%BE/","section":"标签","summary":"","title":"友链","type":"tags"},{"content":"2人目のダミー作者の素晴らしいダミープロフィールです。\n","externalUrl":null,"permalink":"/authors/secondauthor/_index.ja/","section":"作者列表示例","summary":"\u003cp\u003e2人目のダミー作者の素晴らしいダミープロフィールです。\u003c/p\u003e","title":"2人目のダミー作者","type":"authors"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. \u0026#x1f680;\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","externalUrl":null,"permalink":"/tags/advanced/_index.it/","section":"标签","summary":"\u003cp\u003eThis is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. \u0026#x1f680;\u003c/p\u003e","title":"Advanced","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"假装这里有一份第二位作者的简介。\n","externalUrl":null,"permalink":"/authors/secondauthor/","section":"作者列表示例","summary":"\u003cp\u003e假装这里有一份第二位作者的简介。\u003c/p\u003e","title":"Dummy Second Author","type":"authors"},{"content":"Un rapido esempio di come iniziare a utilizzare le tassonomie degli autori nei tuoi articoli.\n","externalUrl":null,"permalink":"/authors/_index.it/","section":"作者列表示例","summary":"\u003cp\u003eUn rapido esempio di come iniziare a utilizzare le tassonomie degli autori nei tuoi articoli.\u003c/p\u003e","title":"Esempio di elenco di tassonomia degli autori","type":"authors"},{"content":"La fantastica biografia di Nuno.\n","externalUrl":null,"permalink":"/authors/nunocoracao/_index.it/","section":"作者列表示例","summary":"\u003cp\u003eLa fantastica biografia di Nuno.\u003c/p\u003e","title":"Nuno Coraçao","type":"authors"},{"content":"Nuno の素晴らしいダミープロフィールです。\n","externalUrl":null,"permalink":"/authors/nunocoracao/_index.ja/","section":"作者列表示例","summary":"\u003cp\u003eNuno の素晴らしいダミープロフィールです。\u003c/p\u003e","title":"Nuno Coração","type":"authors"},{"content":"假装这里有一份 Nuno 的简介。\n","externalUrl":null,"permalink":"/authors/nunocoracao/","section":"作者列表示例","summary":"\u003cp\u003e假装这里有一份 Nuno 的简介。\u003c/p\u003e","title":"Nuno Coração","type":"authors"},{"content":"La fantastica biografia fittizia di Dummy Second Author.\n","externalUrl":null,"permalink":"/authors/secondauthor/_index.it/","section":"作者列表示例","summary":"\u003cp\u003eLa fantastica biografia fittizia di Dummy Second Author.\u003c/p\u003e","title":"Secondo autore fittizio","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"Blowfish ha il pieno supporto per le tassonomie Hugo e si adatterà a qualsiasi impostazione tassonomica. Gli elenchi di tassonomia come questo supportano anche il contenuto personalizzato da visualizzare sopra l\u0026rsquo;elenco dei termini.\nQuest\u0026rsquo;area può essere utilizzata per aggiungere testo descrittivo aggiuntivo a ciascuna tassonomia. Dai un\u0026rsquo;occhiata al tag avanzato di seguito per vedere come portare questo concetto ancora oltre.\n","externalUrl":null,"permalink":"/tags/_index.it/","section":"标签","summary":"\u003cp\u003eBlowfish ha il pieno supporto per le tassonomie Hugo e si adatterà a qualsiasi impostazione tassonomica. Gli elenchi di tassonomia come questo supportano anche il contenuto personalizzato da visualizzare sopra l\u0026rsquo;elenco dei termini.\u003c/p\u003e","title":"Tag","type":"tags"},{"content":"Blowfish は Hugo の分類システムを完全にサポートし、どのような分類システムの設定でも適合できます。このような分類システムのリストは、用語リストの上に表示されるカスタムコンテンツもサポートしています。\nこのエリアはそれぞれの分類システムに追加の説明をする時に利用します。以下の高度なタグを確認して、このコンセプトをさらに発展させる方法をご覧ください。\n","externalUrl":null,"permalink":"/tags/_index.ja/","section":"标签","summary":"\u003cp\u003eBlowfish は Hugo の分類システムを完全にサポートし、どのような分類システムの設定でも適合できます。このような分類システムのリストは、用語リストの上に表示されるカスタムコンテンツもサポートしています。\u003c/p\u003e","title":"タグ","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%8D%9A%E5%AE%A2/","section":"标签","summary":"","title":"博客","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%A4%A7%E5%AD%A6%E4%BF%B1%E4%B9%90%E9%83%A8%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"大学俱乐部网站","type":"tags"},{"content":"こちらは高度なタグです。 Blowfish の他のページの様に、個々の分類条項にカスタムコンテンツの追加やそれを条項リストの一番上に表示することが可能です。\u0026#x1f680;\nタイトルや説明など SEO や他の目的に利用するためのHugo のメタデータを定義するためにこれらのコンテンツページも利用可能です。\n","externalUrl":null,"permalink":"/tags/advanced/_index.ja/","section":"标签","summary":"\u003cp\u003eこちらは高度なタグです。 Blowfish の他のページの様に、個々の分類条項にカスタムコンテンツの追加やそれを条項リストの一番上に表示することが可能です。\u0026#x1f680;\u003c/p\u003e","title":"高度なタグ","type":"tags"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。\u0026#x1f680;\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","externalUrl":null,"permalink":"/tags/advanced/","section":"标签","summary":"\u003cp\u003e这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。\u0026#x1f680;\u003c/p\u003e","title":"高级","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","section":"标签","summary":"","title":"个人博客","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"个人网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%85%AC%E5%8F%B8%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"公司网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/","section":"标签","summary":"","title":"技术博客","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%BB%BA%E6%A8%A1%E5%B8%88/","section":"标签","summary":"","title":"建模师","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E6%BC%AB%E7%94%BB%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"漫画网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E4%BC%81%E4%B8%9A%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"企业网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%BA%BA%E5%91%98/","section":"标签","summary":"","title":"软件开发人员","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E8%A7%86%E9%A2%91%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E5%95%86/","section":"标签","summary":"","title":"视频游戏开发商","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E6%95%B0%E5%AD%97%E8%8A%B1%E5%9B%AD/","section":"标签","summary":"","title":"数字花园","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%A8%83%E5%A8%83%E6%91%84%E5%BD%B1/","section":"标签","summary":"","title":"娃娃摄影","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E9%A1%B9%E7%9B%AE%E7%8E%B0%E5%9C%BA/","section":"标签","summary":"","title":"项目现场","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%AD%A6%E6%9C%AF%E7%95%8C/","section":"标签","summary":"","title":"学术界","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E5%BA%94%E7%94%A8%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"应用网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E6%B8%B8%E6%88%8F%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"游戏网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E6%B8%B8%E6%88%8F%E7%8E%B0%E5%9C%BA/","section":"标签","summary":"","title":"游戏现场","type":"tags"},{"content":"あなたの記事でどのように著者の分類を開始するかの簡単な例です。\n","externalUrl":null,"permalink":"/authors/_index.ja/","section":"作者列表示例","summary":"\u003cp\u003eあなたの記事でどのように著者の分類を開始するかの簡単な例です。\u003c/p\u003e","title":"著者の分類リストの例","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/%E4%B8%BB%E9%A2%98%E4%BD%9C%E8%80%85/","section":"标签","summary":"","title":"主题作者","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E7%BB%84%E7%BB%87%E7%AB%99%E7%82%B9/","section":"标签","summary":"","title":"组织站点","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E4%BD%9C%E5%93%81%E9%9B%86%E7%BD%91%E7%AB%99/","section":"标签","summary":"","title":"作品集网站","type":"tags"},{"content":"","externalUrl":null,"permalink":"/tags/%E4%BD%9C%E8%80%85/","section":"标签","summary":"","title":"作者","type":"tags"},{"content":"在你的文章中添加不同作者的简单示例。\n","externalUrl":null,"permalink":"/authors/","section":"作者列表示例","summary":"\u003cp\u003e在你的文章中添加不同作者的简单示例。\u003c/p\u003e","title":"作者列表示例","type":"authors"}]